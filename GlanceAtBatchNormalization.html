
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="robots" content="" />

  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro|Source+Sans+Pro:300,400,400i,700" rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="https://kentchun33333.github.io/theme/stylesheet/style.min.css">

  <link rel="stylesheet" type="text/css" href="https://kentchun33333.github.io/theme/pygments/github.min.css">
  <link rel="stylesheet" type="text/css" href="https://kentchun33333.github.io/theme/font-awesome/css/font-awesome.min.css">







<meta name="author" content="Kent Chiu" />
<meta name="description" content="A glance at batch normalization" />
<meta name="keywords" content="cnn, training, automatic differenciation, moving average">

<meta property="og:site_name" content="Autumn Memo"/>
<meta property="og:title" content="A Glance at Batch Normalization"/>
<meta property="og:description" content="A glance at batch normalization"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="https://kentchun33333.github.io/GlanceAtBatchNormalization.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2016-11-22 20:00:00+01:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="https://kentchun33333.github.io/author/kent-chiu.html">
<meta property="article:section" content="AI"/>
<meta property="article:tag" content="cnn"/>
<meta property="article:tag" content="training"/>
<meta property="article:tag" content="automatic differenciation"/>
<meta property="article:tag" content="moving average"/>
<meta property="og:image" content="https://scontent.fsin5-1.fna.fbcdn.net/v/t1.0-1/p720x720/96215235_3403513689663854_7453417891073884160_o.jpg?_nc_cat=106&_nc_sid=dbb9e7&_nc_ohc=FFyXABpAhHQAX9b-fUS&_nc_ht=scontent.fsin5-1.fna&_nc_tp=6&oh=3f722a46a66c21fadc044d92033c0590&oe=5EED0413">

  <title>Autumn Memo &ndash; A Glance at Batch Normalization</title>

</head>
<body>
  <aside>
    <div>
      <a href="https://kentchun33333.github.io">
        <img src="https://scontent.fsin5-1.fna.fbcdn.net/v/t1.0-1/p720x720/96215235_3403513689663854_7453417891073884160_o.jpg?_nc_cat=106&_nc_sid=dbb9e7&_nc_ohc=FFyXABpAhHQAX9b-fUS&_nc_ht=scontent.fsin5-1.fna&_nc_tp=6&oh=3f722a46a66c21fadc044d92033c0590&oe=5EED0413" alt="Kent Chiu" title="Kent Chiu">
      </a>
      <h1><a href="https://kentchun33333.github.io">Kent Chiu</a></h1>

<p>Algorithm Mind Space</p>

      <ul class="social">
        <li><a class="sc-linkedin" href="https://www.linkedin.com/in/kent-chiu-93b745a2?trk=hp-identity-photo" target="_blank"><i class="fa fa-linkedin"></i></a></li>
        <li><a class="sc-github" href="https://github.com/KentChun33333" target="_blank"><i class="fa fa-github"></i></a></li>
        <li><a class="sc-facebook" href="https://www.facebook.com/kent.chun" target="_blank"><i class="fa fa-facebook"></i></a></li>
      </ul>
    </div>


  </aside>
  <main>

    <nav>
      <a href="https://kentchun33333.github.io">    Home
</a>

      <a href="/archives.html">Archives</a>
      <a href="/categories.html">Categories</a>
      <a href="/tags.html">Tags</a>


    </nav>

<article class="single">
  <header>
      
    <h1 id="GlanceAtBatchNormalization">A Glance at Batch Normalization</h1>
    <p>
          Posted on Tue 22 November 2016 in <a href="https://kentchun33333.github.io/category/ai.html">AI</a>


    </p>
  </header>


  <div>
    <style type="text/css">/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI escape sequences */
/* The color values are a mix of
   http://www.xcolors.net/dl/baskerville-ivorylight and
   http://www.xcolors.net/dl/euphrasia */
.ansi-black-fg {
  color: #3E424D;
}
.ansi-black-bg {
  background-color: #3E424D;
}
.ansi-black-intense-fg {
  color: #282C36;
}
.ansi-black-intense-bg {
  background-color: #282C36;
}
.ansi-red-fg {
  color: #E75C58;
}
.ansi-red-bg {
  background-color: #E75C58;
}
.ansi-red-intense-fg {
  color: #B22B31;
}
.ansi-red-intense-bg {
  background-color: #B22B31;
}
.ansi-green-fg {
  color: #00A250;
}
.ansi-green-bg {
  background-color: #00A250;
}
.ansi-green-intense-fg {
  color: #007427;
}
.ansi-green-intense-bg {
  background-color: #007427;
}
.ansi-yellow-fg {
  color: #DDB62B;
}
.ansi-yellow-bg {
  background-color: #DDB62B;
}
.ansi-yellow-intense-fg {
  color: #B27D12;
}
.ansi-yellow-intense-bg {
  background-color: #B27D12;
}
.ansi-blue-fg {
  color: #208FFB;
}
.ansi-blue-bg {
  background-color: #208FFB;
}
.ansi-blue-intense-fg {
  color: #0065CA;
}
.ansi-blue-intense-bg {
  background-color: #0065CA;
}
.ansi-magenta-fg {
  color: #D160C4;
}
.ansi-magenta-bg {
  background-color: #D160C4;
}
.ansi-magenta-intense-fg {
  color: #A03196;
}
.ansi-magenta-intense-bg {
  background-color: #A03196;
}
.ansi-cyan-fg {
  color: #60C6C8;
}
.ansi-cyan-bg {
  background-color: #60C6C8;
}
.ansi-cyan-intense-fg {
  color: #258F8F;
}
.ansi-cyan-intense-bg {
  background-color: #258F8F;
}
.ansi-white-fg {
  color: #C5C1B4;
}
.ansi-white-bg {
  background-color: #C5C1B4;
}
.ansi-white-intense-fg {
  color: #A1A6B2;
}
.ansi-white-intense-bg {
  background-color: #A1A6B2;
}
.ansi-default-inverse-fg {
  color: #FFFFFF;
}
.ansi-default-inverse-bg {
  background-color: #000000;
}
.ansi-bold {
  font-weight: bold;
}
.ansi-underline {
  text-decoration: underline;
}
/* The following styles are deprecated an will be removed in a future version */
.ansibold {
  font-weight: bold;
}
.ansi-inverse {
  outline: 0.5px dotted;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  position: relative;
  overflow: visible;
}
div.cell:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: transparent;
}
div.cell.jupyter-soft-selected {
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected,
div.cell.selected.jupyter-soft-selected {
  border-color: #ababab;
}
div.cell.selected:before,
div.cell.selected.jupyter-soft-selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #42A5F5;
}
@media print {
  div.cell.selected,
  div.cell.selected.jupyter-soft-selected {
    border-color: transparent;
  }
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
}
.edit_mode div.cell.selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #66BB6A;
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell > div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area > div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area > div.highlight > pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the <head> if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  /* Note that this should set vertical padding only, since CodeMirror assumes
       that horizontal padding will be set on CodeMirror pre */
  padding: 0.4em 0;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. This sets horizontal padding only,
    use .CodeMirror-lines for vertical */
  padding: 0 0.4em;
  border: 0;
  border-radius: 0;
}
.CodeMirror-cursor {
  border-left: 1.4px solid black;
}
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .CodeMirror-cursor {
    border-left: 2px solid black;
  }
}
@media screen and (min-width: 4320px) {
  .CodeMirror-cursor {
    border-left: 4px solid black;
  }
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area 
div.output_area 
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
div.output_area .mglyph > img {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 1px 0 1px 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}



.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}






.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}
.rendered_html ul:not(.list-inline),
.rendered_html ol:not(.list-inline) {
  padding-left: 2em;
}








.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}





.rendered_html pre,




.rendered_html tr,
.rendered_html th,


.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
.rendered_html * + table {
  margin-top: 1em;
}

.rendered_html * + p {
  margin-top: 1em;
}

.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,

.rendered_html img.unconfined,


.rendered_html * + .alert {
  margin-top: 1em;
}
[dir="rtl"] 
div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell > div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered 
.text_cell.rendered .rendered_html tr,
.text_cell.rendered .rendered_html th,
.text_cell.rendered 
.text_cell.unrendered .text_cell_render {
  display: none;
}
.text_cell .dropzone .input_area {
  border: 2px dashed #bababa;
  margin: -1px;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
</style>
<style type="text/css"> .highlight pre  .hll { background-color: #ffffcc }
 .highlight pre   { background: #f8f8f8; }
 .highlight pre  .c { color: #408080; font-style: italic } /* Comment */
 .highlight pre  .err { border: 1px solid #FF0000 } /* Error */
 .highlight pre  .k { color: #008000; font-weight: bold } /* Keyword */
 .highlight pre  .o { color: #666666 } /* Operator */
 .highlight pre  .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
 .highlight pre  .cm { color: #408080; font-style: italic } /* Comment.Multiline */
 .highlight pre  .cp { color: #BC7A00 } /* Comment.Preproc */
 .highlight pre  .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
 .highlight pre  .c1 { color: #408080; font-style: italic } /* Comment.Single */
 .highlight pre  .cs { color: #408080; font-style: italic } /* Comment.Special */
 .highlight pre  .gd { color: #A00000 } /* Generic.Deleted */
 .highlight pre  .ge { font-style: italic } /* Generic.Emph */
 .highlight pre  .gr { color: #FF0000 } /* Generic.Error */
 .highlight pre  .gh { color: #000080; font-weight: bold } /* Generic.Heading */
 .highlight pre  .gi { color: #00A000 } /* Generic.Inserted */
 .highlight pre  .go { color: #888888 } /* Generic.Output */
 .highlight pre  .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
 .highlight pre  .gs { font-weight: bold } /* Generic.Strong */
 .highlight pre  .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
 .highlight pre  .gt { color: #0044DD } /* Generic.Traceback */
 .highlight pre  .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
 .highlight pre  .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
 .highlight pre  .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
 .highlight pre  .kp { color: #008000 } /* Keyword.Pseudo */
 .highlight pre  .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
 .highlight pre  .kt { color: #B00040 } /* Keyword.Type */
 .highlight pre  .m { color: #666666 } /* Literal.Number */
 .highlight pre  .s { color: #BA2121 } /* Literal.String */
 .highlight pre  .na { color: #7D9029 } /* Name.Attribute */
 .highlight pre  .nb { color: #008000 } /* Name.Builtin */
 .highlight pre  .nc { color: #0000FF; font-weight: bold } /* Name.Class */
 .highlight pre  .no { color: #880000 } /* Name.Constant */
 .highlight pre  .nd { color: #AA22FF } /* Name.Decorator */
 .highlight pre  .ni { color: #999999; font-weight: bold } /* Name.Entity */
 .highlight pre  .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
 .highlight pre  .nf { color: #0000FF } /* Name.Function */
 .highlight pre  .nl { color: #A0A000 } /* Name.Label */
 .highlight pre  .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
 .highlight pre  .nt { color: #008000; font-weight: bold } /* Name.Tag */
 .highlight pre  .nv { color: #19177C } /* Name.Variable */
 .highlight pre  .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
 .highlight pre  .w { color: #bbbbbb } /* Text.Whitespace */
 .highlight pre  .mb { color: #666666 } /* Literal.Number.Bin */
 .highlight pre  .mf { color: #666666 } /* Literal.Number.Float */
 .highlight pre  .mh { color: #666666 } /* Literal.Number.Hex */
 .highlight pre  .mi { color: #666666 } /* Literal.Number.Integer */
 .highlight pre  .mo { color: #666666 } /* Literal.Number.Oct */
 .highlight pre  .sa { color: #BA2121 } /* Literal.String.Affix */
 .highlight pre  .sb { color: #BA2121 } /* Literal.String.Backtick */
 .highlight pre  .sc { color: #BA2121 } /* Literal.String.Char */
 .highlight pre  .dl { color: #BA2121 } /* Literal.String.Delimiter */
 .highlight pre  .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
 .highlight pre  .s2 { color: #BA2121 } /* Literal.String.Double */
 .highlight pre  .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
 .highlight pre  .sh { color: #BA2121 } /* Literal.String.Heredoc */
 .highlight pre  .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
 .highlight pre  .sx { color: #008000 } /* Literal.String.Other */
 .highlight pre  .sr { color: #BB6688 } /* Literal.String.Regex */
 .highlight pre  .s1 { color: #BA2121 } /* Literal.String.Single */
 .highlight pre  .ss { color: #19177C } /* Literal.String.Symbol */
 .highlight pre  .bp { color: #008000 } /* Name.Builtin.Pseudo */
 .highlight pre  .fm { color: #0000FF } /* Name.Function.Magic */
 .highlight pre  .vc { color: #19177C } /* Name.Variable.Class */
 .highlight pre  .vg { color: #19177C } /* Name.Variable.Global */
 .highlight pre  .vi { color: #19177C } /* Name.Variable.Instance */
 .highlight pre  .vm { color: #19177C } /* Name.Variable.Magic */
 .highlight pre  .il { color: #666666 } /* Literal.Number.Integer.Long */</style>
<style type="text/css">
/* Temporary definitions which will become obsolete with Notebook release 5.0 */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }
</style>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Introduction">Introduction<a class="anchor-link" href="#Introduction">&#182;</a></h3><p><strong>Batch normalization</strong> is a popular method to fasten deep-network training process also solving the gradient vanishing or exploding problem. In this <a href="https://kentchun33333.github.io/">post</a>, I am going to first discuss some ideas, then take a glance at math expression on algorithms in original <a href="https://arxiv.org/pdf/1502.03167v3.pdf">paper</a>,and finally take a deep look on how to implement it.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="1.-Brief-concepts-about-normalization">1. Brief concepts about normalization<a class="anchor-link" href="#1.-Brief-concepts-about-normalization">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It is quite common to use normalization in neural network, especially deep convnet. Before batch normalization layer, there are several ways/methods like as per-image-normalize, per-image whitening, per-batch normalization, per-batch whitening, local constrast normalization (LCN) and local response normalization (LRN). These methods are actually really good, and is sucessfully working in some conditions.</p>
<p>To me, the main core ideal is simple, that is <strong> only difference matters while delievering information</strong>. Just think the information like electricity in cpu, nowadays the driving voltage is much lower than the past cpu, but it carried more heavy work, compute even faster and spend less energy.</p>
<p><strong>We actually have multiple ways to normalize our data before or after any operation.</strong> A good way or strategy to add the normalization is actually <strong>depending on the operation before or after it.</strong> For example, if you are going to applied a RELU with a threshod equal to 0.5 ( which is not common) and you would like to add a normalization-method before it. You probabily do not want to only add the normalization but also some shift like 0.1 or 0.3 to make sure this RELU-layer do not swipe too many information. For another example, you would like to introduce a <strong>depth-wise or channel-wise normalization</strong> after the conv-layer. Because, the conv-layer is actually a depth-wish operation. Depth in conv-operation is actually the numbers of filters/kernels which coressponding to a <strong>( </strong>filter-width * filter-height * previous-depth<strong> )</strong> of weights/neurals.</p>
<p>The batch normalization is basically following this concept but adding a trainable feature to it. <strong>This feature makes it beautiful.</strong></p>
<p>For more informations about normalization, check this <a href="http://yeephycho.github.io/2016/08/03/Normalizations-in-neural-networks/">post</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="2.-Expression-in-Math">2. Expression in Math<a class="anchor-link" href="#2.-Expression-in-Math">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>$ \begin{array} \\
\text{Algorithm 1 : Batch Normalizing Transform, applied to activation x over a mini-batch. } \\  
\hline 
\text{Input : Values of x over a mini-Batch : } B \{  x_{1 \text{ ... m }} \} \text{ ; Parameters to be learned : } \beta \text{ and } \gamma \\
\text{Output : A set of } Y : \{ y_{i} = \text{ BatchNorm}_{\beta, \gamma}(x^{i}) \} \\
\text{ } \mu_{ \beta } \leftarrow  \frac{1}{m} \sum_{i=1}^m (x_{i}) \text{ ----------------- mini-batch mean}\\
\text{ } \alpha^{2}_{ \beta } \leftarrow  \frac{1}{m} \sum_{i=1}^m (x_{i} - \mu_{\beta})^{2} \text{ --------- mini-batch variance}\\
\text{ } \hat{x}_{i} \leftarrow \frac{x_{i} - \mu_{\beta}}{\sqrt{\alpha^{2}_{\beta}+ \epsilon}} \text{ ---------------------- normalization where epsilon is the number to prevent dividing zero } \\
\text{ } y_{i} \leftarrow \gamma \hat{x}_{i} + \beta = \text{BN}_{\gamma, \beta}(x_{i}) \text{ -------- scale and shift}\\
\end{array} $</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>$ \begin{array} \\
\text{Algorithm 2 : Training a Batch-Normalized Network } \\  
\hline 
\text{Input : Network N with trainable parameters } \Theta \text{ ; subset of activations } \{ x^{(k)}\}^{K}_{k=1} \\
\text{Output : Batch-normalized network for inference, N}^{inf}_{\text{BN}} \\
\text{ 1: N}^{\text{tr}}_{\text{BN}} \leftarrow \text{ N ------ Training BN network} \\
\text{ 2: } \textbf{for } k = 1 ... K \textbf{  do } : \\ 
\text{ 3: Add transformation } y^{(k)} = \text{BN}_{\gamma^{(k)}, \beta^{(k)}}(x^{(k)}) \text{ to N}^{\text{tr}}_{\text{BN}} \text{ ( Alg. 1 )} \\ 
\text{ 4: Modify each layer in N}^{\text{tr}}_{\text{BN}} \text{ with input } x^{(k)} \text{ to take } y^{(k)} \text{ instead} \\
\text{ 5: } \textbf{end for }\\
\\
\text{ 6: Train N}^{\text{tr}}_{\text{BN}} \text{ to optimize the parameters } \Theta \cup \{ \gamma^{(k)}, \beta^{(k)} \}^{K}_{k=1}\\
\text{ 7: N}^{\text{inf}}_{\text{BN}} \leftarrow \text{N}^{\text{tr}}_{\text{BN}} text{ ---inference BN network with forzen parameters} \\
\\
\text{ 8: } \textbf{for } k = 1 ... K \textbf{  do } : \\ 
\text{ 9: //For clarity} x=x^{(k)}, \gamma = \gamma^{(k)}, \beta = \beta^{(k)} ... etc \\
\text{10: Process multiple training mini-batches B, each of size m, and average over them :}\\
\text{10: E}[x] \leftarrow \text{E}_{\beta}[\mu_{\beta}]\\
\text{10: Var}[x] \leftarrow \frac{m}{m-1} \text{E}_{\beta}[\alpha^{2}_{\beta}]\\
\text{11: In N}^{\text{inf}}_{\text{BN}} \text{, replace the transform } y = \text{BN}_{\gamma, \beta}(x) \text{ with } y = \frac{\gamma}{\sqrt{\text{Var}[x]+\epsilon}} \dot x + (\beta - \frac{\gamma \text{E}[x]}{\sqrt{\text{Var}[x]+\epsilon}})\\
\text{12: } \textbf{end for }\\
\end{array} $</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="3.-Something-you-should-know-before-diving-into-code.">3. Something you should know before diving into code.<a class="anchor-link" href="#3.-Something-you-should-know-before-diving-into-code.">&#182;</a></h3><p>To implement batch normalization is, in fact, really simple, as long as, you know the following things :</p>
<h4 id="3-1-Automatic-Differentiation-:">3-1 Automatic Differentiation :<a class="anchor-link" href="#3-1-Automatic-Differentiation-:">&#182;</a></h4><p>There are basically 4 main methods to compute derivatives or to say <strong>backpropagation</strong>.</p>
<ul>
<li><p>(1) Manually working out derivatives and coding the result</p>
</li>
<li><p>(2) Numerical differentiation (using finite difference approximations)</p>
</li>
<li><p>(3) <strong>Symbolic differentiation</strong> (expression manipulation in SW such as Max, Mathematica &amp; Maple)</p>
</li>
<li><p>(4) <strong>Automatic differentiation</strong> (Forward, Reversed &amp; Optical Jocobian Accumulation)</p>
</li>
</ul>
<p>And most of popular deep learning library had implemented either automatic reverse differenciation or symbolic differenciation while computing gradients and Hessian of an objective fuction.  <strong>Therefore, while implement the batch-normalization layer in real production, we just need to focus on the forward propagation.</strong> (Alg-1)</p>
<p>For more information about automatic differentiation, check this <a href="https://arxiv.org/pdf/1502.05767v2.pdf">paper</a>.</p>
<h4 id="3-2-Moving-Average-&amp;-Variance-in-mini-batch-traning">3-2 Moving Average &amp; Variance in mini-batch traning<a class="anchor-link" href="#3-2-Moving-Average-&amp;-Variance-in-mini-batch-traning">&#182;</a></h4><p>Since we usually use mini-batch traning, <strong>the concept of estimation the population average and variance by moving-methods</strong> is introduced while the batch-normalization is in training state. It just makes an approximation of the suitable mean &amp; variance.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="4.-Batch-normalization-in-Tensorflow.">4. Batch-normalization in Tensorflow.<a class="anchor-link" href="#4.-Batch-normalization-in-Tensorflow.">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The following code is from <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py">tf-contrib-layer.</a> and the test-case could be found in <a href="http://stackoverflow.com/questions/38312668/how-does-one-do-inference-with-batch-normalization-with-tensor-flow">a stackoverflow-issue.</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="highlight"><pre><span></span><span class="nd">@add_arg_scope</span>
<span class="k">def</span> <span class="nf">batch_norm</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span>
               <span class="n">decay</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span>
               <span class="n">center</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
               <span class="n">scale</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
               <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
               <span class="n">activation_fn</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
               <span class="n">updates_collections</span><span class="o">=</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">UPDATE_OPS</span><span class="p">,</span>
               <span class="n">is_training</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
               <span class="n">reuse</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
               <span class="n">variables_collections</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
               <span class="n">outputs_collections</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
               <span class="n">trainable</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
               <span class="n">scope</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Adds a Batch Normalization layer from http://arxiv.org/abs/1502.03167.</span>
<span class="sd">    &quot;Batch Normalization: Accelerating Deep Network Training by Reducing</span>
<span class="sd">    Internal Covariate Shift&quot;</span>
<span class="sd">    Sergey Ioffe, Christian Szegedy</span>
<span class="sd">  Can be used as a normalizer function for conv2d and fully_connected.</span>
<span class="sd">  Args:</span>
<span class="sd">    inputs: a tensor of size `[batch_size, height, width, channels]`</span>
<span class="sd">            or `[batch_size, channels]`.</span>
<span class="sd">    decay: decay for the moving average.</span>
<span class="sd">    center: If True, subtract `beta`. If False, `beta` is ignored.</span>
<span class="sd">    scale: If True, multiply by `gamma`. If False, `gamma` is</span>
<span class="sd">      not used. When the next layer is linear (also e.g. `nn.relu`), this can be</span>
<span class="sd">      disabled since the scaling can be done by the next layer.</span>
<span class="sd">    epsilon: small float added to variance to avoid dividing by zero.</span>
<span class="sd">    activation_fn: Optional activation function.</span>
<span class="sd">    updates_collections: collections to collect the update ops for computation.</span>
<span class="sd">      If None, a control dependency would be added to make sure the updates are</span>
<span class="sd">      computed.</span>
<span class="sd">    is_training: whether or not the layer is in training mode. In training mode</span>
<span class="sd">      it would accumulate the statistics of the moments into `moving_mean` and</span>
<span class="sd">      `moving_variance` using an exponential moving average with the given</span>
<span class="sd">      `decay`. When it is not in training mode then it would use the values of</span>
<span class="sd">      the `moving_mean` and the `moving_variance`.</span>
<span class="sd">    reuse: whether or not the layer and its variables should be reused. To be</span>
<span class="sd">      able to reuse the layer scope must be given.</span>
<span class="sd">    variables_collections: optional collections for the variables.</span>
<span class="sd">    outputs_collections: collections to add the outputs.</span>
<span class="sd">    trainable: If `True` also add variables to the graph collection</span>
<span class="sd">      `GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable).</span>
<span class="sd">    scope: Optional scope for `variable_op_scope`.</span>
<span class="sd">  Returns:</span>
<span class="sd">    a tensor representing the output of the operation.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># =========================================================================</span>
  <span class="c1"># This variable_op_scope is actually depreciate in r1.1 of TF</span>
  <span class="c1"># It basically provide an context-manager for variable_ops</span>
  <span class="c1"># </span>
  <span class="k">with</span> <span class="n">variable_scope</span><span class="o">.</span><span class="n">variable_op_scope</span><span class="p">([</span><span class="n">inputs</span><span class="p">],</span>
                                        <span class="n">scope</span><span class="p">,</span> <span class="s1">&#39;BatchNorm&#39;</span><span class="p">,</span> <span class="n">reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">)</span> <span class="k">as</span> <span class="n">sc</span><span class="p">:</span>
    <span class="n">inputs_shape</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span>

    <span class="c1"># =======================================================================</span>
    <span class="c1"># Base on channel : axis = list(range(len(inputs_shape) - 1)) </span>
    <span class="c1"># -----------------------</span>
    <span class="c1"># However, it is suggested that to use axis as [0,1,2] to tf.nn.moments </span>
    <span class="c1"># </span>
    <span class="n">axis</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs_shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> 
    <span class="n">params_shape</span> <span class="o">=</span> <span class="n">inputs_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:]</span>
    <span class="c1"># Allocate parameters for the beta and gamma of the normalization.</span>
    <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span>


    <span class="c1"># =======================================================================</span>
    <span class="c1"># init the variable according to the mode/state/parameters</span>

    <span class="k">if</span> <span class="n">center</span><span class="p">:</span>
      <span class="n">beta_collections</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">get_variable_collections</span><span class="p">(</span><span class="n">variables_collections</span><span class="p">,</span>
                                                        <span class="s1">&#39;beta&#39;</span><span class="p">)</span>
      <span class="n">beta</span> <span class="o">=</span> <span class="n">variables</span><span class="o">.</span><span class="n">model_variable</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span>
                                      <span class="n">shape</span><span class="o">=</span><span class="n">params_shape</span><span class="p">,</span>
                                      <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
                                      <span class="n">initializer</span><span class="o">=</span><span class="n">init_ops</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">,</span>
                                      <span class="n">collections</span><span class="o">=</span><span class="n">beta_collections</span><span class="p">,</span>
                                      <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">scale</span><span class="p">:</span>
      <span class="n">gamma_collections</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">get_variable_collections</span><span class="p">(</span><span class="n">variables_collections</span><span class="p">,</span>
                                                         <span class="s1">&#39;gamma&#39;</span><span class="p">)</span>
      <span class="n">gamma</span> <span class="o">=</span> <span class="n">variables</span><span class="o">.</span><span class="n">model_variable</span><span class="p">(</span><span class="s1">&#39;gamma&#39;</span><span class="p">,</span>
                                       <span class="n">shape</span><span class="o">=</span><span class="n">params_shape</span><span class="p">,</span>
                                       <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
                                       <span class="n">initializer</span><span class="o">=</span><span class="n">init_ops</span><span class="o">.</span><span class="n">ones_initializer</span><span class="p">,</span>
                                       <span class="n">collections</span><span class="o">=</span><span class="n">gamma_collections</span><span class="p">,</span>
                                       <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">)</span>
    <span class="c1"># =======================================================================</span>
    <span class="c1"># Somve moving method </span>
    <span class="c1"># ---------------------</span>
    <span class="c1"># Create moving_mean and moving_variance variables and add them to the</span>
    <span class="c1"># appropiate collections.</span>

    <span class="n">moving_mean_collections</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">get_variable_collections</span><span class="p">(</span>
        <span class="n">variables_collections</span><span class="p">,</span> <span class="s1">&#39;moving_mean&#39;</span><span class="p">)</span>
    <span class="n">moving_mean</span> <span class="o">=</span> <span class="n">variables</span><span class="o">.</span><span class="n">model_variable</span><span class="p">(</span>
        <span class="s1">&#39;moving_mean&#39;</span><span class="p">,</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">params_shape</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">initializer</span><span class="o">=</span><span class="n">init_ops</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">,</span>
        <span class="n">trainable</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="n">moving_mean_collections</span><span class="p">)</span>
    <span class="n">moving_variance_collections</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">get_variable_collections</span><span class="p">(</span>
        <span class="n">variables_collections</span><span class="p">,</span> <span class="s1">&#39;moving_variance&#39;</span><span class="p">)</span>
    <span class="n">moving_variance</span> <span class="o">=</span> <span class="n">variables</span><span class="o">.</span><span class="n">model_variable</span><span class="p">(</span>
        <span class="s1">&#39;moving_variance&#39;</span><span class="p">,</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">params_shape</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">initializer</span><span class="o">=</span><span class="n">init_ops</span><span class="o">.</span><span class="n">ones_initializer</span><span class="p">,</span>
        <span class="n">trainable</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="n">moving_variance_collections</span><span class="p">)</span>


    <span class="c1"># =======================================================================</span>
    <span class="c1"># mentioned above </span>

    <span class="k">if</span> <span class="n">is_training</span><span class="p">:</span>
      <span class="c1"># =======================================================================</span>
      <span class="c1"># Calculate the moments based on the individual batch.</span>
      <span class="c1"># tf.nn.moments is actually build on top of tf.nn.sufficient_statistics</span>
      <span class="c1"># if using the conv-2d, set axis = [0,1,2] to return depth-wise normalize</span>

      <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">moments</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">shift</span><span class="o">=</span><span class="n">moving_mean</span><span class="p">)</span>

      <span class="c1"># =======================================================================</span>
      <span class="c1"># Update the moving_mean and moving_variance moments.</span>

      <span class="n">update_moving_mean</span> <span class="o">=</span> <span class="n">moving_averages</span><span class="o">.</span><span class="n">assign_moving_average</span><span class="p">(</span>
          <span class="n">moving_mean</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">decay</span><span class="p">)</span>
      <span class="n">update_moving_variance</span> <span class="o">=</span> <span class="n">moving_averages</span><span class="o">.</span><span class="n">assign_moving_average</span><span class="p">(</span>
          <span class="n">moving_variance</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">decay</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">updates_collections</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="c1"># =======================================================================</span>
        <span class="c1"># Make sure the updates are computed here. ( first time )</span>
        <span class="c1"># ======================================================================= </span>
        <span class="c1"># control_dependencies </span>
        <span class="c1"># just make sure compute update first and then compute batch_normalization</span>
        <span class="c1"># </span>
        <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="n">update_moving_mean</span><span class="p">,</span>
                                       <span class="n">update_moving_variance</span><span class="p">]):</span>
          <span class="n">outputs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">batch_normalization</span><span class="p">(</span>
              <span class="n">inputs</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Collect the updates to be computed later.</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">add_to_collections</span><span class="p">(</span><span class="n">updates_collections</span><span class="p">,</span> <span class="n">update_moving_mean</span><span class="p">)</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">add_to_collections</span><span class="p">(</span><span class="n">updates_collections</span><span class="p">,</span> <span class="n">update_moving_variance</span><span class="p">)</span>

        <span class="c1"># =======================================================================</span>
        <span class="c1"># After we get mean, variance, beta, gamma, esilon</span>
        <span class="c1"># we could normalization it (Alg-1, normalization &amp; last line )</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">batch_normalization</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">outputs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">batch_normalization</span><span class="p">(</span>
          <span class="n">inputs</span><span class="p">,</span> <span class="n">moving_mean</span><span class="p">,</span> <span class="n">moving_variance</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
    <span class="n">outputs</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">get_shape</span><span class="p">())</span>

    <span class="c1"># =======================================================================</span>
    <span class="c1"># Additional setting </span>

    <span class="k">if</span> <span class="n">activation_fn</span><span class="p">:</span>
      <span class="n">outputs</span> <span class="o">=</span> <span class="n">activation_fn</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">utils</span><span class="o">.</span><span class="n">collect_named_outputs</span><span class="p">(</span><span class="n">outputs_collections</span><span class="p">,</span> <span class="n">sc</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="5.-Batch-normalization-in-Keras">5. Batch-normalization in Keras<a class="anchor-link" href="#5.-Batch-normalization-in-Keras">&#182;</a></h3><p>There is another implementation from Keras, which is also nice. If you are going to use Batch-normalization in Conv2d with keras, I would recommend use this with parameters <em>mode</em> = 0, <em>axis</em> = 3 or 1, depending on your input tensor  is  for [b, h, w, c] or [b, c, h, w]</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras.engine</span> <span class="kn">import</span> <span class="n">Layer</span><span class="p">,</span> <span class="n">InputSpec</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">initializations</span><span class="p">,</span> <span class="n">regularizers</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">backend</span> <span class="k">as</span> <span class="n">K</span>


<span class="k">class</span> <span class="nc">BatchNormalization</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;Normalize the activations of the previous layer at each batch,</span>
<span class="sd">    i.e. applies a transformation that maintains the mean activation</span>
<span class="sd">    close to 0 and the activation standard deviation close to 1.</span>
<span class="sd">    # Arguments</span>
<span class="sd">        epsilon: small float &gt; 0. Fuzz parameter.</span>
<span class="sd">        mode: integer, 0, 1 or 2.</span>
<span class="sd">            - 0: feature-wise normalization.</span>
<span class="sd">                Each feature map in the input will</span>
<span class="sd">                be normalized separately. The axis on which</span>
<span class="sd">                to normalize is specified by the `axis` argument.</span>
<span class="sd">                Note that if the input is a 4D image tensor</span>
<span class="sd">                using Theano conventions (samples, channels, rows, cols)</span>
<span class="sd">                then you should set `axis` to `1` to normalize along</span>
<span class="sd">                the channels axis.</span>
<span class="sd">                During training we use per-batch statistics to normalize</span>
<span class="sd">                the data, and during testing we use running averages</span>
<span class="sd">                computed during the training phase.</span>
<span class="sd">            - 1: sample-wise normalization. This mode assumes a 2D input.</span>
<span class="sd">            - 2: feature-wise normalization, like mode 0, but</span>
<span class="sd">                using per-batch statistics to normalize the data during both</span>
<span class="sd">                testing and training.</span>
<span class="sd">        axis: integer, axis along which to normalize in mode 0. For instance,</span>
<span class="sd">            if your input tensor has shape (samples, channels, rows, cols),</span>
<span class="sd">            set axis to 1 to normalize per feature map (channels axis).</span>
<span class="sd">        momentum: momentum in the computation of the</span>
<span class="sd">            exponential average of the mean and standard deviation</span>
<span class="sd">            of the data, for feature-wise normalization.</span>
<span class="sd">        weights: Initialization weights.</span>
<span class="sd">            List of 2 Numpy arrays, with shapes:</span>
<span class="sd">            `[(input_shape,), (input_shape,)]`</span>
<span class="sd">            Note that the order of this list is [gamma, beta, mean, std]</span>
<span class="sd">        beta_init: name of initialization function for shift parameter</span>
<span class="sd">            (see [initializations](../initializations.md)), or alternatively,</span>
<span class="sd">            Theano/TensorFlow function to use for weights initialization.</span>
<span class="sd">            This parameter is only relevant if you don&#39;t pass a `weights` argument.</span>
<span class="sd">        gamma_init: name of initialization function for scale parameter (see</span>
<span class="sd">            [initializations](../initializations.md)), or alternatively,</span>
<span class="sd">            Theano/TensorFlow function to use for weights initialization.</span>
<span class="sd">            This parameter is only relevant if you don&#39;t pass a `weights` argument.</span>
<span class="sd">        gamma_regularizer: instance of [WeightRegularizer](../regularizers.md)</span>
<span class="sd">            (eg. L1 or L2 regularization), applied to the gamma vector.</span>
<span class="sd">        beta_regularizer: instance of [WeightRegularizer](../regularizers.md),</span>
<span class="sd">            applied to the beta vector.</span>
<span class="sd">    # Input shape</span>
<span class="sd">        Arbitrary. Use the keyword argument `input_shape`</span>
<span class="sd">        (tuple of integers, does not include the samples axis)</span>
<span class="sd">        when using this layer as the first layer in a model.</span>
<span class="sd">    # Output shape</span>
<span class="sd">        Same shape as input.</span>
<span class="sd">    # References</span>
<span class="sd">        - [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://jmlr.org/proceedings/papers/v37/ioffe15.pdf)</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span>
                 <span class="n">weights</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">beta_init</span><span class="o">=</span><span class="s1">&#39;zero&#39;</span><span class="p">,</span> <span class="n">gamma_init</span><span class="o">=</span><span class="s1">&#39;one&#39;</span><span class="p">,</span>
                 <span class="n">gamma_regularizer</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">beta_regularizer</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">supports_masking</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta_init</span> <span class="o">=</span> <span class="n">initializations</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">beta_init</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma_init</span> <span class="o">=</span> <span class="n">initializations</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">gamma_init</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">momentum</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma_regularizer</span> <span class="o">=</span> <span class="n">regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">gamma_regularizer</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta_regularizer</span> <span class="o">=</span> <span class="n">regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">beta_regularizer</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initial_weights</span> <span class="o">=</span> <span class="n">weights</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">uses_learning_phase</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BatchNormalization</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="c1"># =======================================================================</span>
    <span class="c1"># some init , similar to tf implementation</span>
    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_spec</span> <span class="o">=</span> <span class="p">[</span><span class="n">InputSpec</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">)]</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">],)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma_init</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;{}_gamma&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta_init</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;{}_beta&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trainable_weights</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">regularizers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma_regularizer</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gamma_regularizer</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">regularizers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gamma_regularizer</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta_regularizer</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">beta_regularizer</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">regularizers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">beta_regularizer</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span>
                                    <span class="n">name</span><span class="o">=</span><span class="s1">&#39;{}_running_mean&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">running_std</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span>
                                  <span class="n">name</span><span class="o">=</span><span class="s1">&#39;{}_running_std&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">non_trainable_weights</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_std</span><span class="p">]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">initial_weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">initial_weights</span><span class="p">)</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">initial_weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">built</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">called_with</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">built</span><span class="p">,</span> <span class="s1">&#39;Layer must be built before being called&#39;</span>
            <span class="n">input_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_spec</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>

            <span class="n">reduction_axes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)))</span>
            <span class="k">del</span> <span class="n">reduction_axes</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">]</span>
            <span class="n">broadcast_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
            <span class="n">broadcast_shape</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">]</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">]</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">x_normed</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">std</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">normalize_batch_in_training</span><span class="p">(</span>
                    <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">,</span> <span class="n">reduction_axes</span><span class="p">,</span>
                    <span class="n">epsilon</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># mode 0</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">called_with</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">{</span><span class="bp">None</span><span class="p">,</span> <span class="n">x</span><span class="p">}:</span>
                    <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s1">&#39;You are attempting to share a &#39;</span>
                                    <span class="s1">&#39;same `BatchNormalization` layer across &#39;</span>
                                    <span class="s1">&#39;different data flows. &#39;</span>
                                    <span class="s1">&#39;This is not possible. &#39;</span>
                                    <span class="s1">&#39;You should use `mode=2` in &#39;</span>
                                    <span class="s1">&#39;`BatchNormalization`, which has &#39;</span>
                                    <span class="s1">&#39;a similar behavior but is shareable &#39;</span>
                                    <span class="s1">&#39;(see docs for a description of &#39;</span>
                                    <span class="s1">&#39;the behavior).&#39;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">called_with</span> <span class="o">=</span> <span class="n">x</span>
                <span class="n">x_normed</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">std</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">normalize_batch_in_training</span><span class="p">(</span>
                    <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">,</span> <span class="n">reduction_axes</span><span class="p">,</span>
                    <span class="n">epsilon</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">updates</span> <span class="o">=</span> <span class="p">[</span><span class="n">K</span><span class="o">.</span><span class="n">moving_average_update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span><span class="p">),</span>
                                <span class="n">K</span><span class="o">.</span><span class="n">moving_average_update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">running_std</span><span class="p">,</span> <span class="n">std</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span><span class="p">)]</span>

                <span class="k">if</span> <span class="n">K</span><span class="o">.</span><span class="n">backend</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;tensorflow&#39;</span> <span class="ow">and</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">reduction_axes</span><span class="p">)</span> <span class="o">==</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">ndim</span><span class="p">(</span><span class="n">x</span><span class="p">))[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
                <span class="c1"># =======================================================================</span>
                <span class="c1"># Alg-1 : normalization </span>

                    <span class="n">x_normed_running</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">batch_normalization</span><span class="p">(</span>
                        <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_std</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">,</span>
                        <span class="n">epsilon</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># need broadcasting</span>
                    <span class="n">broadcast_running_mean</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span><span class="p">,</span> <span class="n">broadcast_shape</span><span class="p">)</span>
                    <span class="n">broadcast_running_std</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">running_std</span><span class="p">,</span> <span class="n">broadcast_shape</span><span class="p">)</span>
                    <span class="n">broadcast_beta</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">,</span> <span class="n">broadcast_shape</span><span class="p">)</span>
                    <span class="n">broadcast_gamma</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">,</span> <span class="n">broadcast_shape</span><span class="p">)</span>
                    <span class="n">x_normed_running</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">batch_normalization</span><span class="p">(</span>
                        <span class="n">x</span><span class="p">,</span> <span class="n">broadcast_running_mean</span><span class="p">,</span> <span class="n">broadcast_running_std</span><span class="p">,</span>
                        <span class="n">broadcast_beta</span><span class="p">,</span> <span class="n">broadcast_gamma</span><span class="p">,</span>
                        <span class="n">epsilon</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">)</span>

                <span class="c1"># pick the normalized form of x corresponding to the training phase</span>
                <span class="n">x_normed</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">in_train_phase</span><span class="p">(</span><span class="n">x_normed</span><span class="p">,</span> <span class="n">x_normed_running</span><span class="p">)</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># sample-wise normalization</span>
            <span class="n">m</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
            <span class="n">std</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">)</span>
            <span class="n">x_normed</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">m</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">std</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">)</span>
            <span class="n">x_normed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">x_normed</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span>
        <span class="k">return</span> <span class="n">x_normed</span>

    <span class="k">def</span> <span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;epsilon&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">,</span>
                  <span class="s1">&#39;mode&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="p">,</span>
                  <span class="s1">&#39;axis&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">,</span>
                  <span class="s1">&#39;gamma_regularizer&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma_regularizer</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma_regularizer</span> <span class="k">else</span> <span class="bp">None</span><span class="p">,</span>
                  <span class="s1">&#39;beta_regularizer&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta_regularizer</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta_regularizer</span> <span class="k">else</span> <span class="bp">None</span><span class="p">,</span>
                  <span class="s1">&#39;momentum&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span><span class="p">}</span>
        <span class="n">base_config</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">BatchNormalization</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
        <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">base_config</span><span class="o">.</span><span class="n">items</span><span class="p">())</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">items</span><span class="p">()))</span>
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="6.-Summary">6. Summary<a class="anchor-link" href="#6.-Summary">&#182;</a></h3><p>Batch-normalization is a nice trick. Expecially for a fully convnet. 
If you like my post, you can star my <a href="https://github.com/KentChun33333/kentchun33333.github.io">github project</a> or consider to buy me a coffe. Thank you so much : )</p>

</div>
</div>
</div>
 


<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['$','$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        " linebreaks: { automatic: true, width: '95% container' }, " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>

  </div>
  <div class="tag-cloud">
    <p>
      <a href="https://kentchun33333.github.io/tag/cnn.html">cnn</a>
      <a href="https://kentchun33333.github.io/tag/training.html">training</a>
      <a href="https://kentchun33333.github.io/tag/automatic-differenciation.html">automatic differenciation</a>
      <a href="https://kentchun33333.github.io/tag/moving-average.html">moving average</a>
    </p>
  </div>





<!-- Disqus -->
<div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'https-kentchun33333-github-io';
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>
        Please enable JavaScript to view comments.

</noscript>
<!-- End Disqus -->
</article>

    <footer>
<p>&copy;  </p>
<p>    Powered by <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a>
</p>    </footer>
  </main>




<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Autumn Memo ",
  "url" : "https://kentchun33333.github.io",
  "image": "https://scontent.fsin5-1.fna.fbcdn.net/v/t1.0-1/p720x720/96215235_3403513689663854_7453417891073884160_o.jpg?_nc_cat=106&_nc_sid=dbb9e7&_nc_ohc=FFyXABpAhHQAX9b-fUS&_nc_ht=scontent.fsin5-1.fna&_nc_tp=6&oh=3f722a46a66c21fadc044d92033c0590&oe=5EED0413",
  "description": "Just Memos"
}
</script>

</body>
</html>
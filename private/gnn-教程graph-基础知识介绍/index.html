<!doctype html>
<html lang="en">
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <meta name="referrer" content="no-referrer-when-downgrade">
    

    <title> | Autumn Memo</title>
    <meta property="og:title" content=" - Autumn Memo">
    <meta property="og:type" content="article">
        
        
    <meta name="Keywords" content="Life Design, AI, Machine Learning, Problem Solving">
    <meta name="description" content="">
        
    <meta name="author" content="Kent Chiu">
    <meta property="og:url" content="https://kentchun33333.github.io/private/gnn-%E6%95%99%E7%A8%8Bgraph-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E4%BB%8B%E7%BB%8D/">
    <link rel="shortcut icon" href='/favicon.ico'  type="image/x-icon">

    <link rel="stylesheet" href='/css/normalize.css'>
    <link rel="stylesheet" href='/css/style.css'>
    <script type="text/javascript" src="//cdn.bootcdn.net/ajax/libs/jquery/3.4.1/jquery.min.js"></script>

    
    
    
    
    
    
</head>


<body>
    <header id="header" class="clearfix">
    <div class="container">
        <div class="col-group">
            <div class="site-name ">
                
                    <a id="logo" href="https://kentchun33333.github.io">
                        Autumn Memo
                    </a>
                
                <p class="description">A place recording the core memo.</p>
            </div>
            <div>
                <nav id="nav-menu" class="clearfix">
                    <a class="" href="https://kentchun33333.github.io">Head Page</a>
                    
                    <a  href="https://kentchun33333.github.io/archives/" title="Archives">Archives</a>
                    
                    <a  href="https://kentchun33333.github.io/about/" title="About">About</a>
                    
                </nav>
            </div>
        </div>
    </div>
</header>

    <div id="body">
        <div class="container">
            <div class="col-group">

                <div class="col-8" id="main">
                    
<div class="res-cons">
    <style type="text/css">
    .post-toc {
        position: fixed;
        width: 200px;
        margin-left: -210px;
        padding: 5px 10px;
        font-family: Athelas, STHeiti, Microsoft Yahei, serif;
        font-size: 12px;
        border: 1px solid rgba(0, 0, 0, .07);
        border-radius: 5px;
        background-color: rgba(255, 255, 255, 0.98);
        background-clip: padding-box;
        -webkit-box-shadow: 1px 1px 2px rgba(0, 0, 0, .125);
        box-shadow: 1px 1px 2px rgba(0, 0, 0, .125);
        word-wrap: break-word;
        white-space: nowrap;
        -webkit-box-sizing: border-box;
        box-sizing: border-box;
        z-index: 999;
        cursor: pointer;
        max-height: 70%;
        overflow-y: auto;
        overflow-x: hidden;
    }

    .post-toc .post-toc-title {
        width: 100%;
        margin: 0 auto;
        font-size: 20px;
        font-weight: 400;
        text-transform: uppercase;
        text-align: center;
    }

    .post-toc .post-toc-content {
        font-size: 15px;
    }

    .post-toc .post-toc-content>nav>ul {
        margin: 10px 0;
    }

    .post-toc .post-toc-content ul {
        padding-left: 20px;
        list-style: square;
        margin: 0.5em;
        line-height: 1.8em;
    }

    .post-toc .post-toc-content ul ul {
        padding-left: 15px;
        display: none;
    }

    @media print,
    screen and (max-width:1057px) {
        .post-toc {
            display: none;
        }
    }
</style>
<div class="post-toc" style="position: absolute; top: 188px;">
    <h2 class="post-toc-title"> Tree</h2>
    <div class="post-toc-content">
        <nav id="TableOfContents">
  <ul>
    <li><a href="#引言">引言</a></li>
    <li><a href="#一欧几里得结构化数据">一、欧几里得结构化数据</a>
      <ul>
        <li><a href="#1欧几里得空间">1、欧几里得空间</a></li>
        <li><a href="#2-常见的欧几里得结构化数据">2、 常见的欧几里得结构化数据</a></li>
      </ul>
    </li>
    <li><a href="#二非欧几里得结构化数据">二、非欧几里得结构化数据</a>
      <ul>
        <li><a href="#1非欧几里得空间">1、非欧几里得空间</a></li>
        <li><a href="#2-非常见的欧几里得结构化数据">2、 非常见的欧几里得结构化数据</a></li>
      </ul>
    </li>
    <li><a href="#三图graph">三、图(Graph)</a>
      <ul>
        <li><a href="#1图graph的引入">1、图(Graph)的引入</a></li>
        <li><a href="#2图graph的定义">2、图(Graph)的定义</a></li>
        <li><a href="#3图graph的表示形式">3、图(Graph)的表示形式</a></li>
      </ul>
    </li>
    <li><a href="#四图上的学习任务">四、图上的学习任务</a></li>
    <li><a href="#五图数据应用举例">五、图数据应用举例</a></li>
    <li><a href="#六系列规划">六、系列规划</a></li>
    <li><a href="#参考资料">参考资料</a></li>
    <li><a href="#引言-1">引言</a></li>
    <li><a href="#卷积层的设计">卷积层的设计</a>
      <ul>
        <li><a href="#graphconv">GraphConv</a></li>
        <li><a href="#relgraphconv">RelGraphConv</a></li>
        <li><a href="#tagconv">TAGConv</a></li>
        <li><a href="#gatconv">GATConv</a></li>
        <li><a href="#pointconv">PointConv</a></li>
        <li><a href="#edgeconv">EdgeConv</a></li>
        <li><a href="#sageconv">SAGEConv</a></li>
        <li><a href="#sgconv">SGConv</a></li>
        <li><a href="#appnpconv">APPNPConv</a></li>
        <li><a href="#ginconv">GINConv</a></li>
        <li><a href="#gatedgraphconv">GatedGraphConv</a></li>
        <li><a href="#gmmconv">GMMConv</a></li>
        <li><a href="#chebconv">ChebConv</a></li>
        <li><a href="#agnnconv">AGNNConv</a></li>
        <li><a href="#nnconv">NNConv</a></li>
        <li><a href="#dnaconv">DNAConv</a></li>
      </ul>
    </li>
    <li><a href="#池化层的设计">池化层的设计</a>
      <ul>
        <li><a href="#sumpooling">SumPooling</a></li>
        <li><a href="#avgpooling">AvgPooling</a></li>
        <li><a href="#maxpooling">MaxPooling</a></li>
        <li><a href="#sortpooling">SortPooling</a></li>
        <li><a href="#topkpooling">TopKPooling</a></li>
        <li><a href="#sagpooling">SAGPooling</a></li>
        <li><a href="#globalattentionpooling">GlobalAttentionPooling</a></li>
        <li><a href="#set2set">Set2Set</a></li>
        <li><a href="#settrasformerencoder--settrasformerdecoder">SetTrasformerEncoder &amp; SetTrasformerDecoder</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#引言-2">引言</a></li>
    <li><a href="#问题定义">问题定义</a></li>
    <li><a href="#图上的快速卷积近似">图上的快速卷积近似</a>
      <ul>
        <li><a href="#谱图卷积spectral-graph-convolutions">谱图卷积(Spectral Graph Convolutions)</a></li>
        <li><a href="#逐层线性模型">逐层线性模型</a></li>
      </ul>
    </li>
    <li><a href="#半监督学习节点分类">半监督学习节点分类</a></li>
    <li><a href="#传播公式解释">传播公式解释</a>
      <ul>
        <li><a href="#例子">例子</a></li>
      </ul>
    </li>
    <li><a href="#后话">后话</a></li>
    <li><a href="#reference">Reference</a></li>
  </ul>
</nav>
    </div>
</div>
<script type="text/javascript">
    $(document).ready(function () {
        var postToc = $(".post-toc");
        if (postToc.length) {
            var leftPos = $("#main").offset().left;
            if(leftPos<220){
                postToc.css({"width":leftPos-10,"margin-left":(0-leftPos)})
            }

            var t = postToc.offset().top - 20,
                a = {
                    start: {
                        position: "absolute",
                        top: t
                    },
                    process: {
                        position: "fixed",
                        top: 20
                    },
                };
            $(window).scroll(function () {
                var e = $(window).scrollTop();
                e < t ? postToc.css(a.start) : postToc.css(a.process)
            })
        }
    })
</script>
    <article class="post">
        <header>
            <h1 class="post-title"></h1>
        </header>
        <date class="post-meta meta-date">
            1 - 1 - 1 
        </date>
        
        
        <div class="post-meta">
            <span id="busuanzi_container_page_pv">|<span id="busuanzi_value_page_pv"></span><span>
                    Reading </span></span>
        </div>
        
        
        <div class="post-content">
            <h1 id="gnn-教程graph-基础知识介绍">GNN 教程：Graph 基础知识介绍</h1>
<p><a href="https://archwalker.github.io/blog/2019/05/31/GNN-basics.html">https://archwalker.github.io/blog/2019/05/31/GNN-basics.html</a></p>
<ul>
<li>
<p><a href="https://archwalker.github.io/archive.html?tag=GNN">GNN</a></p>
</li>
<li>
<p>May 31, 2019</p>
</li>
<li>
<p>0 views</p>
</li>
</ul>
<h2 id="引言">引言</h2>
<p>图卷积神经网络(Graph Convolutional Network)作为最近几年兴起的一种基于图结构的广义神经网络结构，因为其独特的计算能力，而受到广泛学者的关注与研究。传统深度学习模型 LSTM 和 CNN 在欧几里得空间数据(语言，图像，视频等)上取得了不错的成绩，但是在对非欧几里得空间数据(eg：社交网络、信息网络等)进行处理上却存在一定的局限性。</p>
<p>针对该问题，研究者们引入了图论中抽象意义上的图(Graph)来表示非欧几里得结构化数据。并利用图卷积网络对来图(Graph)数据进行处理，以深入发掘其特征和规律。</p>
<p>本文首先分别介绍了欧几里得结构化数据和非欧几里得结构化数据特点；然后，针对非欧几里得结构化数据的表示问题，引入了图论中抽象意义上的图(Graph)概念，并对图(Graph)中一些表示形式进行介绍；最后，通过一个简单的例子，对图(Graph)数据的应用进行介绍。以帮助读者加深对图(Graph)的理解。</p>
<h2 id="一欧几里得结构化数据">一、欧几里得结构化数据</h2>
<h3 id="1欧几里得空间">1、欧几里得空间</h3>
<p>欧几里德空间(Euclidean Space)，简称为欧氏空间(也可以称为平直空间)，在数学中是对欧几里德所研究的2维和3维空间的一般化。这个一般化把欧几里德对于距离、以及相关的概念长度和角度，转换成任意数维的坐标系。如下图所示。</p>
<p>
        <img class="mx-auto" alt="euclidean space" src="http://ww4.sinaimg.cn/large/006tNc79ly1g46vmxsp95j30lu0b3adf.jpg" />   
    </p>
<blockquote>
<p>图 a 表示二维欧几里得空间</p>
</blockquote>
<blockquote>
<p>图 b 表示三维欧几里得空间</p>
</blockquote>
<h3 id="2-常见的欧几里得结构化数据">2、 常见的欧几里得结构化数据</h3>
<p>将数据转换到欧几里得空间中，所得到的数据称为<strong>欧几里得结构化数据</strong>。</p>
<p>常见的欧几里得结构化数据主要包含：</p>
<ul>
<li>1D：声音，时间序列等；</li>
<li>2D：图像等；</li>
<li>3D：视频，高光谱图像等；</li>
</ul>
<p>
        <img class="mx-auto" alt="euclidean space data" src="http://ww4.sinaimg.cn/large/006tNc79ly1g46vmukxctj30l8090q5a.jpg" />   
    </p>
<h2 id="二非欧几里得结构化数据">二、非欧几里得结构化数据</h2>
<h3 id="1非欧几里得空间">1、非欧几里得空间</h3>
<p>然而，科学研究中并不是所有的数据都能够被转换到欧几里得空间中(eg：社交网络、信息网络等)，对于不能进行欧几里得结构化的数据，我们将其称为非欧几里得结构化数据。</p>
<h3 id="2-非常见的欧几里得结构化数据">2、 非常见的欧几里得结构化数据</h3>
<p>常见的非欧几里得结构化数据主要包含：</p>
<ul>
<li>1D：社交网络(eg：Facebook，Twitter等)等；</li>
<li>2D：生物网络(基因，分子，大脑连接)等；</li>
<li>3D：基础设施网络(eg：能源，交通，互联网，通信等)等；</li>
</ul>
<p>
        <img class="mx-auto" alt="no euclidean space data" src="http://ww1.sinaimg.cn/large/006tNc79ly1g46vm87j37j30r807841t.jpg" />   
    </p>
<h2 id="三图graph">三、图(Graph)</h2>
<h3 id="1图graph的引入">1、图(Graph)的引入</h3>
<p>针对非欧几里得结构化数据表示问题，研究者们引入了图论中抽象意义上的图(Graph)来表示非欧几里得结构化数据。</p>
<h3 id="2图graph的定义">2、图(Graph)的定义</h3>
<p>图(Graph)定义形式为G=(V,E)G=(V,E)，其结构如下图所示：</p>
<p>
        <img class="mx-auto" alt="graph" src="http://ww3.sinaimg.cn/large/006tNc79ly1g46vmn1lj8j30c8083q30.jpg" />   
    </p>
<blockquote>
<p>一个有标号的简单图，点集V={1,2,3,4,5,6}V={1,2,3,4,5,6}，边集E={{1,2},{1,5},{2,3},{2,5},{3,4},{4,5},{4,6}}E={{1,2},{1,5},{2,3},{2,5},{3,4},{4,5},{4,6}}</p>
</blockquote>
<p>V={vi|i=1,…,N}V={vi|i=1,…,N} 表示顶点或节点, 其中NN表示节点的个数。 E={eij|vi,vj∈V},|E|≤N2E={eij|vi,vj∈V},|E|≤N2 表示顶点与顶点之间所连接的边；</p>
<h3 id="3图graph的表示形式">3、图(Graph)的表示形式</h3>
<h4 id="1邻接矩阵adjacency-matrix">(1)、邻接矩阵(Adjacency matrix)</h4>
<p>邻接矩阵是一个元素为bool值或权值的N×NN×N矩阵，该矩阵的定义如下：</p>
<p>A∈RN×N,Aij={aij≠0eij∈E0, otherwise (1)(1)A∈RN×N,Aij={aij≠0eij∈E0, otherwise</p>
<p>若图中存在一条连接顶点vivi和vjvj的边eijeij，则aij≠0aij≠0,否则为0。当图是稠密时，邻接矩阵是比较合适的表达方法。如下图所示：</p>
<pre><code>+---+---+---+---+---+---+---+
|   | 1 | 2 | 3 | 4 | 5 | 6 |
+---+---+---+---+---+---+---+
| 1 | 0 | 1 | 0 | 0 | 1 | 0 |
+---+---+---+---+---+---+---+
| 2 | 1 | 0 | 1 | 0 | 0 | 0 |
+---+---+---+---+---+---+---+
| 3 | 0 | 1 | 0 | 1 | 0 | 0 |
+---+---+---+---+---+---+---+
| 4 | 0 | 0 | 1 | 0 | 1 | 1 |
+---+---+---+---+---+---+---+
| 5 | 1 | 0 | 0 | 1 | 0 | 0 |
+---+---+---+---+---+---+---+
| 6 | 0 | 0 | 0 | 1 | 0 | 0 |
+---+---+---+---+---+---+---+
</code></pre><blockquote>
<p>上图的邻接矩阵表示</p>
</blockquote>
<h4 id="2度矩阵-degree-matrix">(2)、度矩阵( Degree matrix)</h4>
<p>度矩阵( Degree matrix)是一个 Di,iDi,i 为节点vivi的度的对角矩阵，其定义如下所示： D∈RN×N,Dii=∑jAijD∈RN×N,Dii=∑jAij</p>
<pre><code>+---+---+---+---+---+---+---+
|   | 1 | 2 | 3 | 4 | 5 | 6 |
+---+---+---+---+---+---+---+
| 1 | 2 |   |   |   |   |   |
+---+---+---+---+---+---+---+
| 2 |   | 3 |   |   |   |   |
+---+---+---+---+---+---+---+
| 3 |   |   | 2 |   |   |   |
+---+---+---+---+---+---+---+
| 4 |   |   |   | 3 |   |   |
+---+---+---+---+---+---+---+
| 5 |   |   |   |   | 3 |   |
+---+---+---+---+---+---+---+
| 6 |   |   |   |   |   | 1 |
+---+---+---+---+---+---+---+
</code></pre><blockquote>
<p>上图的度矩阵表示</p>
</blockquote>
<h4 id="3邻域-neighborhood">(3)、邻域( Neighborhood)</h4>
<p>邻域( Neighborhood) 表示与某个顶点有边连接的点集，其定义如下所示： N(vi)={vj|eij∈E}N(vi)={vj|eij∈E} 例如，节点11的领域为{2,5}{2,5}</p>
<h2 id="四图上的学习任务">四、图上的学习任务</h2>
<p>介绍完图的基本术语之后，我们来看看有了图结构数据，我们可以进行哪些机器学习的任务</p>
<ul>
<li>图节点分类任务：图中每个节点都有对应的特征，当我们已知一些节点的类别的时候，可以设计分类任务针对未知节点进行分类。我们接下来要介绍的 GCN、GraphSAGE、GAT模型都是对图上的节点分类。</li>
<li>图边结构预测任务：图中的节点和节点之间的边关系可能在输入数据中能够采集到，而有些隐藏的边需要我们挖掘出来，这类任务就是对边的预测任务，也就是对节点和节点之间关系的预测。</li>
<li>图的分类：对于整个图来说，我们也可以对图分类，图分类又称为图的同构问题，基本思路是将图中节点的特征聚合起来作为图的特征，再进行分类。</li>
</ul>
<h2 id="五图数据应用举例">五、图数据应用举例</h2>
<p>对于一个简单的电商的图，其包含卖家，商品和用户三个关键节点，其中，商品节点关联商品类别节点，用户节点关联注册 IP 节点和 注册地址节点。当用户在购买商品时，用户节点和商品节点就会关联交易节点，同时，交易节点也会关联用户下单时所对应的 IP 节点以及收获地址节点，对应的图结构如下图所示。</p>
<p>
        <img class="mx-auto" alt="img" src="http://ww1.sinaimg.cn/large/006tNc79ly1g4vq7jl1afj30m408kwen.jpg" />   
    </p>
<p>从图数据中节点间的关系以及特征，我们可以进行反欺诈以及商品推荐的操作。</p>
<ol>
<li>节点分类—反欺诈：因为图中每个节点都拥有自己的特征信息。通过该特征信息，我们可以构建一个风控系统，如果交易节点所关联的用户 IP 和收货地址与用户注册 IP 和注册地址不匹配，那么系统将有可能认为该用户存在欺诈风险。</li>
<li>边结构预测—商品推荐：图中每个节点都具有结构信息。如果用户频繁购买某种类别商品或对某种类别商品评分较高，那么系统就可以认定该用户对该类商品比较感兴趣，所以就可以向该用户推荐更多该类别的商品。</li>
</ol>
<p>总而言之，图数据的丰富应用价值促使更多的研究者加入图数据的研究当中，但是对图数据进行数据分析时，我们需要同时考虑到节点的特征信息以及结构信息。如果靠手工规则来提取，必将失去很多隐蔽和复杂的模式，那么有没有一种方法能自动化地同时学到图的特征信息与结构信息呢？这就是近年来兴起的机器学习的一个热点方向—图神经网络（Graph Neural Networks）。接下来我们将以一个系列的文章介绍它们。</p>
<h2 id="六系列规划">六、系列规划</h2>
<p>本文为GNN教程 [第一章 基础：三剑客] 的第一篇文章 [01 基础知识]，下图展示了我们在这一系列的规划，接下来我们将会介绍图神经网络的三个基本模型，使大家对他们有所了解。</p>
<p>
        <img class="mx-auto" alt="Screen Shot 2019-07-11 at 11.33.30 AM" src="http://ww1.sinaimg.cn/large/006tNc79ly1g4vqx6po03j30s40quwgz.jpg" />   
    </p>
<h2 id="参考资料">参考资料</h2>
<p><a href="https://www.ezlippi.com/blog/2014/11/graph.html">https://www.ezlippi.com/blog/2014/11/graph.html</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/58792104">https://zhuanlan.zhihu.com/p/58792104</a></p>
<p><a href="https://archwalker.github.io/feed.xml">Subscribe</a></p>
<p>This work is licensed under a <a href="https://creativecommons.org/licenses/by-nc/4.0/">Attribution-NonCommercial 4.0 International</a> license.</p>
<h2 id="引言-1">引言</h2>
<p><strong>此为原创文章，未经许可，禁止转载</strong></p>
<p>GNN的各种模型在近两年来非常火热，在各个会议、期刊上新的模型层出不穷，他们有的做了理论创新，有的对前人的工作提出了改进，在这篇博文中，我想要带大家回顾GNN在近两年来的一些模型的异同，着重体现在他们的数学表达式上的差异。</p>
<p>这篇博文主要遵循 <a href="https://docs.dgl.ai/api/python/nn.pytorch.html">DGL</a> 框架和<a href="https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#module-torch_geometric.nn.conv.message_passing">PyTorch geometric</a>的梳理脉络，加上一些对公式以及背后思想的解释。这篇博文面向的读者是对图神经网络已经有了一定程度的了解的学者。</p>
<p>文章中整理的GNN模型只是目前提出各种创新的一小部分，欢迎大家补充其他的模型。才疏学浅，如有疏漏，欢迎大家指正，可以通过github pull request 的方式，也可以留言或者发邮件给我，谢谢！</p>
<h2 id="卷积层的设计">卷积层的设计</h2>
<h3 id="graphconv">GraphConv</h3>
<p>来自论文<a href="https://arxiv.org/abs/1609.02907">Semi-Supervised Classification with Graph Convolutional Networks (ICLR 2017)</a> 一作是<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Kipf%2C+T+N">Thomas N. Kipf</a>，他当时在Amsterdam大学读博士。</p>
<p>这篇论文是可谓是图神经网络的开山之作，在我们前序的<a href="https://archwalker.github.io/blog/2019/06/01/GNN-Triplets-GCN.html">博文</a>中也有解析，文中提出了一个简单且有效的图卷积算法：</p>
<p>h(l+1)i=σ(b(l)+∑j∈N(i)1cijh(l)jW(l))(1)(1)hi(l+1)=σ(b(l)+∑j∈N(i)1cijhj(l)W(l))</p>
<p>其中 N(i)N(i) 表示节点ii的邻居节点，cij=√|N(i)|√|N(j)|cij=|N(i)||N(j)| 是正则化项，这个公式的思想很简单，节点ii更新后的Embedding为邻居节点Embedding的加权表示。在论文的实现中，W(l)W(l) 以<em>Glorot uniform initialization</em> 的方式初始化，b(l)b(l)被初始化为0。</p>
<h3 id="relgraphconv">RelGraphConv</h3>
<p>来自论文 <a href="https://arxiv.org/abs/1703.06103">Modeling Relational Data with Graph Convolutional Networks (ESWC 2018 Best Student Research Paper)</a> <a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Kipf%2C+T+N">Thomas N. Kipf</a>仍是并列一作。</p>
<p>这篇论文主要解决的问题是如果图数据有不同种类的边那么该如何进行卷积，作者提出的做法是对不同种类的边进行加权求和处理：</p>
<p>h(l+1)i=σ(∑r∈R∑j∈Nr(i)1ci,rW(l)rh(l)j+W(l)0h(l)i)(2)(2)hi(l+1)=σ(∑r∈R∑j∈Nr(i)1ci,rWr(l)hj(l)+W0(l)hi(l))</p>
<p>式中 Nr(i)Nr(i) 表示在边类型为rr时节点ii的邻居节点，ci,r=|Nr(i)|ci,r=|Nr(i)|是正则化项，W(l)r=∑Bb=1a(l)rbV(l)bWr(l)=∑b=1Barb(l)Vb(l) 是对WrWr的基向量分解，这样分解的原因是如果有太多种的边类型的时候，通过分解，只需要学习基向量V(l)bVb(l)，减少欠拟合的风险。</p>
<h3 id="tagconv">TAGConv</h3>
<p>来自论文<a href="https://arxiv.org/pdf/1710.10370.pdf">Topology Adaptive Graph Convolutional Networks</a> 一作是来自CMU的Jian Du(杜建)，他当时在CMU做postdoc。</p>
<p>这篇论文是将GCN中对卷积的简化做了部分还原，细节在我们之前关于谱图卷积的理论<a href="https://archwalker.github.io/blog/2019/06/16/GNN-Spectral-Graph.html">博文</a>中介绍过，具体而言，GCN模型是对卷积核进行Chebyshev多项式近似后取k=1k=1，这篇论文提出的方法将变量kk保留下来作为超参：</p>
<p>X′=K∑k=0D−1/2AD−1/2XΘk(3)(3)X′=∑k=0KD−1/2AD−1/2XΘk</p>
<p>与上面的记号稍有不同，这个公式用的是矩阵的更新形式，其中Dii=∑j=0AijDii=∑j=0Aij表示节点ii的度。</p>
<h3 id="gatconv">GATConv</h3>
<p>来自论文 <a href="https://arxiv.org/pdf/1710.10903.pdf">Graph Attention Network (ICLR 2018)</a> 也是GNN各种模型中一个比较知名的模型，在我们之前的<a href="https://archwalker.github.io/blog/2019/06/01/GNN-Triplets-GAT.html">博文</a>中介绍过，一作是剑桥大学的Petar Velickovic，这篇文章是在Yoshua Bengio的指导下完成的。</p>
<p>论文的核心思想是对邻居的重要性进行学习，利用学习到的重要性权重进行加权求和再对自身Embedding更新：</p>
<p>h(l+1)i=∑j∈N(i)αi,jW(l)h(l)j(4)(4)hi(l+1)=∑j∈N(i)αi,jW(l)hj(l)</p>
<p>其中 αi,jαi,j是邻居jj对节点ii的相对重要性权重，是通过下式学习得到的：</p>
<p>αlij=softmaxi(elij)elij=LeakyReLU(→aT[Whi|Whj])(5)(5)αijl=softmaxi(eijl)eijl=LeakyReLU(a→T[Whi|Whj])</p>
<p>值得一提的是，同年的ICLR上，还有一篇关于Graph Attention的论文 <a href="https://arxiv.org/abs/1803.03735">Attention-based Graph Neural Network for Semi-supervised Learning</a> 文章的主要思想是根据当前节点和邻居节点Embedding的cosine相似度作为Attention的加权因子，做了详细的实验和分析。</p>
<h3 id="pointconv">PointConv</h3>
<p>来自论文 <a href="https://arxiv.org/abs/1612.00593">PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</a> 这是较早得将GNN应用在点云上的文章：</p>
<p>x′i=γΘ(maxj∈N(i)∪{i}hΘ(xj,pj−pi)),(6)(6)xi′=γΘ(maxj∈N(i)∪{i}hΘ(xj,pj−pi)),</p>
<p>其中γΘγΘ和hΘhΘ都表示多层感知机层，P∈RN×DP∈RN×D 表示每个点的坐标向量。</p>
<h3 id="edgeconv">EdgeConv</h3>
<p>来自论文 <a href="https://arxiv.org/pdf/1801.07829">Dynamic Graph CNN for Learning on Point Clouds (TOG 2019)</a> 第一作者是MIT的博士Yue Wang, 这是另一一篇将图神经网络应用在点云上的文章，对于邻居节点Embedding的汇聚方法，他们是这么定义的：</p>
<p>x(l+1)i=maxj∈N(i)ReLU(Θ⋅(x(l)j−x(l)i)+Φ⋅x(l)i)(7)(7)xi(l+1)=maxj∈N(i)ReLU(Θ⋅(xj(l)−xi(l))+Φ⋅xi(l))</p>
<p>这里使用两个节点Embedding的差在点云的数据上有其场景意义，因为在点云的数据集上，节点的Embedding一般取的是节点的坐标向量，所以两个节点Embedding的差表示的是两个坐标向量的差，即自当前节点ii出发，到其邻居节点jj的向量。</p>
<h3 id="sageconv">SAGEConv</h3>
<p>来自论文 <a href="https://arxiv.org/pdf/1706.02216.pdf">Inductive Representation Learning on Large Graphs</a> 第一作者是Stanford的博士<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hamilton%2C+W+L">William L. Hamilton</a> 这篇文章是一篇非常经典的文章，里面提到的采样汇聚的方法也是目前将图神经网络应用到大规模数据集上的基础，我们在之前的<a href="https://archwalker.github.io/blog/2019/06/01/GNN-Triplets-GraphSAGE.html">博文</a>中也对其进行了详细的介绍：</p>
<p>
        <img class="mx-auto" alt="image-20210711172749563" src="/Users/kentchiu/KC-Research-Lab/doc/SkillSet/GNN-202107.assets/image-20210711172749563.png" />   
    </p>
<p>文章的思想如下，因为图数据每个节点的邻居个数是不一定的，带来了计算上的一些困难，所以文章通过采样的方法确保每个节点参与汇聚的邻居个数一定。在公式中jj是采样得到的节点之一，aggregate函数对采样得到的节点集合进行汇聚，邻居集合的汇总Embedding和节点ii本身的Embedding拼起来，再经过转换得到节点ii更新后的Embedding。</p>
<h3 id="sgconv">SGConv</h3>
<p>来自论文 <a href="https://arxiv.org/pdf/1902.07153.pdf">Simplifying Graph Convolutional Networks (ICML 2019)</a> 第一作者是来自Cornell的<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wu%2C+F">Felix Wu</a> ，文章通过实验发现不需要在每个卷积层进行线性变化和激活，这些操作可以合在一起做：</p>
<p>Hl+1=(^D−1/2^A^D−1/2)KHlΘl(9)(9)Hl+1=(D^−1/2A^D^−1/2)KHlΘl</p>
<p>这里H(l+1)H(l+1)可以直接作为节点Embedding的输出结果。个人认为使用H(l+1)H(l+1)表达是不准确的，这样写好像在每一层都需要这个卷积操作，反而加大了计算量，没有达到论文中“简化”的目的。</p>
<h3 id="appnpconv">APPNPConv</h3>
<p>来自论文 <a href="https://arxiv.org/pdf/1810.05997.pdf">Predict then Propagate: Graph Neural Networks meet Personalized PageRank</a> ，文章试着将节点Embedding用一个类似于PageRank的框架更新：</p>
<p>H0=XHt+1=(1−α)(^D−1/2^A^D−1/2Ht+αH0)(10)(10)H0=XHt+1=(1−α)(D^−1/2A^D^−1/2Ht+αH0)</p>
<p>即每个节点的Embedding更新时会包含一部分历史的Embedding。</p>
<h3 id="ginconv">GINConv</h3>
<p>来自论文 <a href="https://arxiv.org/pdf/1810.00826.pdf">How Powerful are Graph Neural Networks? (ICLR 2019 oral)</a>，这是一篇比较有名的文章，作者是来自MIT的xukeyulu，这篇论文算是比较早的想要从理论上分析GNN模型的表达能力的文章，在我们的<a href="https://archwalker.github.io/blog/2019/06/22/GNN-Theory-Power.html">博文</a>中有详细的介绍，文章想要研究GNN在图同构测试中能力，通过和Weisfeiler-Leman对比，得到一个具有和Weisfeiler-Leman想当能力的图神经网络模型：</p>
<p>h(l+1)i=fΘ((1+ϵ)hli+aggregate({hlj,j∈N(i)}))(11)(11)hi(l+1)=fΘ((1+ϵ)hil+aggregate({hjl,j∈N(i)}))</p>
<p>这篇论文主要做了两点创新，第一，公式中的aggregate采用add而非大部分GNN模型中的mean pooling，第二，给节点自身的Embedding加了少许扰动ϵϵ。后来的很多GNN模型都采用了该论文提出的方法作为子模块。</p>
<h3 id="gatedgraphconv">GatedGraphConv</h3>
<p>来自论文 <a href="https://arxiv.org/pdf/1511.05493.pdf">Gated Graph Sequence Neural Networks</a>，这篇论文是一篇早期的探索图神经网络中的长依赖的论文，一作是来自多伦多大学的Yujia Li，论文利用了时序建模中的GRU模块：</p>
<p>h0i=[xi|0]ati=∑j∈N(i)Weijhtjht+1i=GRU(ati,hti)(12)(12)hi0=[xi|0]ait=∑j∈N(i)Weijhjthit+1=GRU(ait,hit)</p>
<p>可以看到，和之前介绍的GNN模型不同，邻居节点汇聚后Embedding atiait不再直接加到自身Embedding上(GCN)，也不再直接concat到自身Embedding上(GraphSAGE)，而是采用GRU的方式汇聚，以保持对长依赖的建模。</p>
<h3 id="gmmconv">GMMConv</h3>
<p>来自论文 <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Monti_Geometric_Deep_Learning_CVPR_2017_paper.pdf">Geometric Deep Learning on Graphs and Manifolds using Mixture Model CNNs (CVPR 2017)</a> ，论文提出了一个叫MoNet的框架，这个框架我不是特别熟悉，故暂且仅留下公式：</p>
<p>hl+1i=aggregate({1KK∑kwk(uij),∀j∈N(i)})wk(u)=exp(−12(u−μk)TΣ−1k(u−μk))(13)(13)hil+1=aggregate({1K∑kKwk(uij),∀j∈N(i)})wk(u)=exp⁡(−12(u−μk)TΣk−1(u−μk))</p>
<h3 id="chebconv">ChebConv</h3>
<p>来自论文 <a href="https://arxiv.org/pdf/1606.09375.pdf">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</a>，这是对于谱图卷积的切比雪夫多项式近似，细节在我们之前关于谱图卷积的理论<a href="https://archwalker.github.io/blog/2019/06/16/GNN-Spectral-Graph.html">博文</a>中介绍过，公式为：</p>
<p>hl+1i=K−1∑k=0Wk,lzk,liZ0,l=HlZ1,l=^L⋅HlZk,l=2⋅^L⋅Zk−1,l−Zk−2,l^L=2(I−^D−1/2^A^D−1/2)/λmax−I(14)(14)hil+1=∑k=0K−1Wk,lzik,lZ0,l=HlZ1,l=L^⋅HlZk,l=2⋅L^⋅Zk−1,l−Zk−2,lL^=2(I−D^−1/2A^D^−1/2)/λmax−I</p>
<h3 id="agnnconv">AGNNConv</h3>
<p>来自论文 <a href="https://arxiv.org/abs/1803.03735">Attention-based Graph Neural Network for Semi-Supervised Learning</a>，上文中介绍过，这篇论文和Graph Attention Network 一起投稿在ICLR 2018上，这篇论文的主要思想是通过余弦相似度计算邻居节点和当前节点的加权权重，但是这篇论文最终被拒了，有可能是因为提出的方法比较简单，不过论文中做了详尽的实验分析，还是值得一看的：</p>
<p>Hl+1=PHlPij=softmaxi(β⋅cos(hli,hlj))(15)(15)Hl+1=PHlPij=softmaxi(β⋅cos⁡(hil,hjl))</p>
<h3 id="nnconv">NNConv</h3>
<p>来自论文 <a href="https://arxiv.org/pdf/1704.01212.pdf">Neural Message Passing for Quantum Chemistry</a>，主要用来解决边上有权重的图神经网络改如何更新Embedding的问题，提出的架构为：</p>
<p>hl+1i=hli+aggregate({fΘ(eij)⋅hlj,j∈N(i)})(16)(16)hil+1=hil+aggregate({fΘ(eij)⋅hjl,j∈N(i)})</p>
<p>其中边上的权重eijeij被显示的建模到模型中来，作为邻居加权求和的权重，这里免去了对邻居权重的学习，直接用边的权重表征邻居的相对重要性。</p>
<h3 id="dnaconv">DNAConv</h3>
<p>来自论文 <a href="https://arxiv.org/abs/1904.04849">Just Jump: Towards Dynamic Neighborhood Aggregation in Graph Neural Networks</a> ，也着重于邻居相对重要性，与Graph Attention Network不同，这篇论文的方法更加直接得将“QKV”式的attention引入到公式中：</p>
<p>x(t)v=h(t)Θ(x(t)v←v,{x(t)v←w:w∈N(v)})(17)(17)xv(t)=hΘ(t)(xv←v(t),{xv←w(t):w∈N(v)})</p>
<p>其中邻居节点的Embedding由”QKV”式的attention计算：</p>
<p>x(t)v←w=Attention(x(t−1)vΘ(t)Q,[x(1)w,…,x(t−1)w]Θ(t)K,[x(1)w,…,x(t−1)w]Θ(t)V)(18)(18)xv←w(t)=Attention(xv(t−1)ΘQ(t),[xw(1),…,xw(t−1)]ΘK(t),[xw(1),…,xw(t−1)]ΘV(t))</p>
<p>其中 Θ(t)Q,Θ(t)K,Θ(t)VΘQ(t),ΘK(t),ΘV(t) 分别代表query, key 和 value 的隐射矩阵。</p>
<h2 id="池化层的设计">池化层的设计</h2>
<p>池化层其实就是上文中提到的各个aggregator，用来将邻居的Embedding聚合起来生成一个汇总的Embedding再和节点自身的Embedding进行操作。</p>
<h3 id="sumpooling">SumPooling</h3>
<p>顾名思义，将邻居Embedding的每一维求和：</p>
<p>r(i)=Ni∑k=1x(i)k(19)(19)r(i)=∑k=1Nixk(i)</p>
<p>其中x(i)kxk(i)表示邻居kk Embedding的第ii维。</p>
<h3 id="avgpooling">AvgPooling</h3>
<p>将邻居Embedding的每一维求均值：</p>
<p>r(i)=1NiNi∑k=1x(i)k(20)(20)r(i)=1Ni∑k=1Nixk(i)</p>
<h3 id="maxpooling">MaxPooling</h3>
<p>将邻居Embedding的按每一维取最大值：</p>
<p>r(i)=Nimaxk=1(x(i)k)(21)(21)r(i)=maxk=1Ni(xk(i))</p>
<h3 id="sortpooling">SortPooling</h3>
<p>来自论文 <a href="https://www.cse.wustl.edu/~ychen/public/DGCNN.pdf">An End-to-End Deep Learning Architecture for Graph Classification</a> ，这篇文章的主要思路是通过 <a href="https://archwalker.github.io/blog/2019/06/22/GNN-Theory-WL.html">WL算法</a> 可以对节点进行着色，而节点的颜色可以定义节点之间的次序，有了节点的次序，我们就可以通过1-D卷积的方法进行卷积运算。因为过程比较抽象，具体请参考原论文。</p>
<h3 id="topkpooling">TopKPooling</h3>
<p>来自论文 <a href="https://arxiv.org/abs/1905.05178">Graph U-Nets</a> ，论文的主要思路是将节点Embedding隐射到1维空间中选择其中top k个节点再进行图卷积的计算，以下是构造top k节点小图的选取过程：</p>
<p>y=Xp∥p∥i=topk(y)X′=(X⊙tanh(y))iA′=Ai,i(22)(22)y=Xp‖p‖i=topk(y)X′=(X⊙tanh(y))iA′=Ai,i</p>
<p>也可以设置一个阈值 ~αα~：</p>
<p>y=softmax(Xp)i=yi&gt;~αX′=(X⊙y)iA′=Ai,i,(23)(23)y=softmax(Xp)i=yi&gt;α~X′=(X⊙y)iA′=Ai,i,</p>
<h3 id="sagpooling">SAGPooling</h3>
<p>来自论文 <a href="https://arxiv.org/abs/1904.08082">Self-Attention Graph Pooling</a>，论文可以看做是TopKPooling的衍生工作，在TopKPooling中，节点的排序因子yy按照线性映射的方式生成，而在这篇文章的工作中，作者是用GNN的方法来生成节点排序因子：</p>
<p>y=GNN(X,A)i=topk(y)X′=(X⊙tanh(y))iA′=Ai,i(24)(24)y=GNN(X,A)i=topk(y)X′=(X⊙tanh(y))iA′=Ai,i</p>
<p>对应的设置阈值~αα~的版本：</p>
<p>y=softmax(GNN(X,A))i=yi&gt;~αX′=(X⊙y)iA′=Ai,i,(25)(25)y=softmax(GNN(X,A))i=yi&gt;α~X′=(X⊙y)iA′=Ai,i,</p>
<h3 id="globalattentionpooling">GlobalAttentionPooling</h3>
<p>来自论文 <a href="https://arxiv.org/abs/1511.05493.pdf">Gated Graph Sequence Neural Networks</a> 对图中所有节点进行pooling，主要用来做对整个图的分类等任务：</p>
<p>r(i)=Ni∑k=1softmax(fgate(x(i)k))ffeat(x(i)k)(26)(26)r(i)=∑k=1Nisoftmax(fgate(xk(i)))ffeat(xk(i))</p>
<p>其中r(i)r(i) 被称作“读出器”，是对于整个图的Embedding描述。</p>
<h3 id="set2set">Set2Set</h3>
<p>来自论文 <a href="https://arxiv.org/pdf/1511.06391.pdf">Order Matters: Sequence to sequence for sets</a>，和Graph attention 有点类似，其中加权求和的权重是通过LSTM建模得到的：</p>
<p>qt=LSTM(q∗t−1)αi,t=softmax(xi⋅qt)rt=N∑i=1αi,txiq∗t=qt∥rt(27)(27)qt=LSTM(qt−1∗)αi,t=softmax(xi⋅qt)rt=∑i=1Nαi,txiqt∗=qt‖rt</p>
<h3 id="settrasformerencoder--settrasformerdecoder">SetTrasformerEncoder &amp; SetTrasformerDecoder</h3>
<p>来自论文 <a href="https://arxiv.org/pdf/1810.00825.pdf">Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks</a>，是Set2Set的衍生模型，公式也有点复杂，可以查下原论文。</p>
<p>GraphSAGE</p>
<p>SAGEConv 接收七个参数：</p>
<ul>
<li>in_feat：输入的特征大小，可以是一个整型数，也可以是两个整型数。如果用在单向二部图上，则可以用整型数来分别表示源节点和目的节点的特征大小，如果只给一个的话，则默认源节点和目的节点的特征大小一致。需要注意的是，如果参数 aggregator_type 为 gcn 的话，则源节点和目的节点的特征大小必须一致；</li>
<li>out_feats：输出特征大小；</li>
<li>aggregator_type：聚合类型，目前支持 mean、gcn、pool、lstm，比论文多一个 gcn 聚合，gcn 聚合可以理解为周围所有的邻居结合和当前节点的均值；</li>
<li>feat_drop=0.：特征 drop 的概率，默认为 0；</li>
<li>bias=True：输出层的 bias，默认为 True；</li>
<li>norm=None：归一化，可以选择一个归一化的方式，默认为 None</li>
<li>activation=None：激活函数，可以选择一个激活函数去更新节点特征，默认为 None。</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SAGEConv</span>(nn<span style="color:#f92672">.</span>Module):
    <span style="color:#66d9ef">def</span> __init__(self,
                 in_feats,
                 out_feats,
                 aggregator_type,
                 feat_drop<span style="color:#f92672">=</span><span style="color:#ae81ff">0.</span>,
                 bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
                 norm<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>,
                 activation<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
        super(SAGEConv, self)<span style="color:#f92672">.</span>__init__()

        <span style="color:#75715e"># expand_as_pair 函数可以返回一个二维元组。</span>
        self<span style="color:#f92672">.</span>_in_src_feats, self<span style="color:#f92672">.</span>_in_dst_feats <span style="color:#f92672">=</span> expand_as_pair(in_feats)
        self<span style="color:#f92672">.</span>_out_feats <span style="color:#f92672">=</span> out_feats
        self<span style="color:#f92672">.</span>_aggre_type <span style="color:#f92672">=</span> aggregator_type
        self<span style="color:#f92672">.</span>norm <span style="color:#f92672">=</span> norm
        self<span style="color:#f92672">.</span>feat_drop <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Dropout(feat_drop)
        self<span style="color:#f92672">.</span>activation <span style="color:#f92672">=</span> activation
        <span style="color:#75715e"># aggregator type: mean/pool/lstm/gcn</span>
        <span style="color:#66d9ef">if</span> aggregator_type <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;pool&#39;</span>:
            self<span style="color:#f92672">.</span>fc_pool <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(self<span style="color:#f92672">.</span>_in_src_feats, self<span style="color:#f92672">.</span>_in_src_feats)
        <span style="color:#66d9ef">if</span> aggregator_type <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;lstm&#39;</span>:
            self<span style="color:#f92672">.</span>lstm <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LSTM(self<span style="color:#f92672">.</span>_in_src_feats, self<span style="color:#f92672">.</span>_in_src_feats, batch_first<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
        <span style="color:#66d9ef">if</span> aggregator_type <span style="color:#f92672">!=</span> <span style="color:#e6db74">&#39;gcn&#39;</span>:
            self<span style="color:#f92672">.</span>fc_self <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(self<span style="color:#f92672">.</span>_in_dst_feats, out_feats, bias<span style="color:#f92672">=</span>bias)
        self<span style="color:#f92672">.</span>fc_neigh <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(self<span style="color:#f92672">.</span>_in_src_feats, out_feats, bias<span style="color:#f92672">=</span>bias)
        self<span style="color:#f92672">.</span>reset_parameters()

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">reset_parameters</span>(self):
        <span style="color:#e6db74">&#34;&#34;&#34;初始化参数
</span><span style="color:#e6db74">        这里的 gain 可以从 calculate_gain 中获取针对非线形激活函数的建议的值
</span><span style="color:#e6db74">        用于初始化参数
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        gain <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>calculate_gain(<span style="color:#e6db74">&#39;relu&#39;</span>)
        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>_aggre_type <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;pool&#39;</span>:
            nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>xavier_uniform_(self<span style="color:#f92672">.</span>fc_pool<span style="color:#f92672">.</span>weight, gain<span style="color:#f92672">=</span>gain)
        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>_aggre_type <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;lstm&#39;</span>:
            self<span style="color:#f92672">.</span>lstm<span style="color:#f92672">.</span>reset_parameters()
        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>_aggre_type <span style="color:#f92672">!=</span> <span style="color:#e6db74">&#39;gcn&#39;</span>:
            nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>xavier_uniform_(self<span style="color:#f92672">.</span>fc_self<span style="color:#f92672">.</span>weight, gain<span style="color:#f92672">=</span>gain)
        nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>xavier_uniform_(self<span style="color:#f92672">.</span>fc_neigh<span style="color:#f92672">.</span>weight, gain<span style="color:#f92672">=</span>gain)

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_lstm_reducer</span>(self, nodes):
        <span style="color:#e6db74">&#34;&#34;&#34;LSTM reducer
</span><span style="color:#e6db74">        NOTE(zihao): lstm reducer with default schedule (degree bucketing)
</span><span style="color:#e6db74">        is slow, we could accelerate this with degree padding in the future.
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        m <span style="color:#f92672">=</span> nodes<span style="color:#f92672">.</span>mailbox[<span style="color:#e6db74">&#39;m&#39;</span>] <span style="color:#75715e"># (B, L, D)</span>
        batch_size <span style="color:#f92672">=</span> m<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
        h <span style="color:#f92672">=</span> (m<span style="color:#f92672">.</span>new_zeros((<span style="color:#ae81ff">1</span>, batch_size, self<span style="color:#f92672">.</span>_in_src_feats)),
             m<span style="color:#f92672">.</span>new_zeros((<span style="color:#ae81ff">1</span>, batch_size, self<span style="color:#f92672">.</span>_in_src_feats)))
        _, (rst, _) <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lstm(m, h)
        <span style="color:#66d9ef">return</span> {<span style="color:#e6db74">&#39;neigh&#39;</span>: rst<span style="color:#f92672">.</span>squeeze(<span style="color:#ae81ff">0</span>)}

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, graph, feat):
        <span style="color:#e6db74">&#34;&#34;&#34; SAGE 层的前向传播
</span><span style="color:#e6db74">        接收 DGLGraph 和 Tensor 格式的节点特征
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        <span style="color:#75715e"># local_var 会返回一个作用在内部函数中使用的 Graph 对象</span>
        <span style="color:#75715e"># 外部数据的变化不会影响到这个 Graph</span>
        <span style="color:#75715e"># 可以理解为保护数据不被意外修改</span>
        graph <span style="color:#f92672">=</span> graph<span style="color:#f92672">.</span>local_var()

        <span style="color:#66d9ef">if</span> isinstance(feat, tuple):
            feat_src <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>feat_drop(feat[<span style="color:#ae81ff">0</span>])
            feat_dst <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>feat_drop(feat[<span style="color:#ae81ff">1</span>])
        <span style="color:#66d9ef">else</span>:
            feat_src <span style="color:#f92672">=</span> feat_dst <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>feat_drop(feat)

        h_self <span style="color:#f92672">=</span> feat_dst

        <span style="color:#75715e"># 根据不同的聚合类型选择不同的聚合方式</span>
        <span style="color:#75715e"># 值得注意的是，论文在 3.3 节只给出了三种聚合方式</span>
        <span style="color:#75715e"># 而这里却多出来一个 gcn 聚合</span>
        <span style="color:#75715e"># 具体原因将在后文给出</span>
        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>_aggre_type <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;mean&#39;</span>:
            graph<span style="color:#f92672">.</span>srcdata[<span style="color:#e6db74">&#39;h&#39;</span>] <span style="color:#f92672">=</span> feat_src
            graph<span style="color:#f92672">.</span>update_all(fn<span style="color:#f92672">.</span>copy_src(<span style="color:#e6db74">&#39;h&#39;</span>, <span style="color:#e6db74">&#39;m&#39;</span>), fn<span style="color:#f92672">.</span>mean(<span style="color:#e6db74">&#39;m&#39;</span>, <span style="color:#e6db74">&#39;neigh&#39;</span>))
            h_neigh <span style="color:#f92672">=</span> graph<span style="color:#f92672">.</span>dstdata[<span style="color:#e6db74">&#39;neigh&#39;</span>]
        <span style="color:#66d9ef">elif</span> self<span style="color:#f92672">.</span>_aggre_type <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;gcn&#39;</span>:
            <span style="color:#75715e"># check_eq_shape 用于检查源节点和目的节点的特征大小是否一致</span>
            check_eq_shape(feat)
            graph<span style="color:#f92672">.</span>srcdata[<span style="color:#e6db74">&#39;h&#39;</span>] <span style="color:#f92672">=</span> feat_src
            graph<span style="color:#f92672">.</span>dstdata[<span style="color:#e6db74">&#39;h&#39;</span>] <span style="color:#f92672">=</span> feat_dst     <span style="color:#75715e"># same as above if homogeneous</span>
            graph<span style="color:#f92672">.</span>update_all(fn<span style="color:#f92672">.</span>copy_src(<span style="color:#e6db74">&#39;h&#39;</span>, <span style="color:#e6db74">&#39;m&#39;</span>), fn<span style="color:#f92672">.</span>sum(<span style="color:#e6db74">&#39;m&#39;</span>, <span style="color:#e6db74">&#39;neigh&#39;</span>))
            <span style="color:#75715e"># divide in_degrees</span>
            degs <span style="color:#f92672">=</span> graph<span style="color:#f92672">.</span>in_degrees()<span style="color:#f92672">.</span>to(feat_dst)
            h_neigh <span style="color:#f92672">=</span> (graph<span style="color:#f92672">.</span>dstdata[<span style="color:#e6db74">&#39;neigh&#39;</span>] <span style="color:#f92672">+</span> graph<span style="color:#f92672">.</span>dstdata[<span style="color:#e6db74">&#39;h&#39;</span>]) <span style="color:#f92672">/</span> (degs<span style="color:#f92672">.</span>unsqueeze(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>) <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)
        <span style="color:#66d9ef">elif</span> self<span style="color:#f92672">.</span>_aggre_type <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;pool&#39;</span>:
            graph<span style="color:#f92672">.</span>srcdata[<span style="color:#e6db74">&#39;h&#39;</span>] <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>fc_pool(feat_src))
            graph<span style="color:#f92672">.</span>update_all(fn<span style="color:#f92672">.</span>copy_src(<span style="color:#e6db74">&#39;h&#39;</span>, <span style="color:#e6db74">&#39;m&#39;</span>), fn<span style="color:#f92672">.</span>max(<span style="color:#e6db74">&#39;m&#39;</span>, <span style="color:#e6db74">&#39;neigh&#39;</span>))
            h_neigh <span style="color:#f92672">=</span> graph<span style="color:#f92672">.</span>dstdata[<span style="color:#e6db74">&#39;neigh&#39;</span>]
        <span style="color:#66d9ef">elif</span> self<span style="color:#f92672">.</span>_aggre_type <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;lstm&#39;</span>:
            graph<span style="color:#f92672">.</span>srcdata[<span style="color:#e6db74">&#39;h&#39;</span>] <span style="color:#f92672">=</span> feat_src
            graph<span style="color:#f92672">.</span>update_all(fn<span style="color:#f92672">.</span>copy_src(<span style="color:#e6db74">&#39;h&#39;</span>, <span style="color:#e6db74">&#39;m&#39;</span>), self<span style="color:#f92672">.</span>_lstm_reducer)
            h_neigh <span style="color:#f92672">=</span> graph<span style="color:#f92672">.</span>dstdata[<span style="color:#e6db74">&#39;neigh&#39;</span>]
        <span style="color:#66d9ef">else</span>:
            <span style="color:#66d9ef">raise</span> <span style="color:#a6e22e">KeyError</span>(<span style="color:#e6db74">&#39;Aggregator type </span><span style="color:#e6db74">{}</span><span style="color:#e6db74"> not recognized.&#39;</span><span style="color:#f92672">.</span>format(self<span style="color:#f92672">.</span>_aggre_type))

        <span style="color:#75715e"># GraphSAGE GCN does not require fc_self.</span>
        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>_aggre_type <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;gcn&#39;</span>:
            rst <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>fc_neigh(h_neigh)
        <span style="color:#66d9ef">else</span>:
            rst <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>fc_self(h_self) <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>fc_neigh(h_neigh)
        <span style="color:#75715e"># activation</span>
        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>activation <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
            rst <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>activation(rst)
        <span style="color:#75715e"># normalization</span>
        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>norm <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
            rst <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>norm(rst)
        <span style="color:#66d9ef">return</span> rst
</code></pre></div><h1 id="gnn-教程gcn">GNN 教程：GCN</h1>
<ul>
<li>
<p><a href="https://archwalker.github.io/archive.html?tag=GNN">GNN</a></p>
</li>
<li>
<p>Jun 01, 2019</p>
</li>
<li>
<p>0 views</p>
</li>
</ul>
<p><strong>此为原创文章，转载务必保留<a href="https://archwalker.github.io/">出处</a></strong></p>
<h2 id="引言-2">引言</h2>
<p>这是我们介绍图神经网络的第一篇文章，取自<a href="http://arxiv.org/abs/1609.02907">Kipf et al. 2017</a>，文章中提出的模型叫Graph Convolutional Network(GCN)，个人认为可以看作是图神经网络的“开山之作”，因为GCN利用了近似的技巧推导出了一个简单而高效的模型，使得图像处理中的卷积操作能够简单得被用到图结构数据处理中来，后面各种图神经网络层出不穷，或多或少都受到这篇文章的启发。</p>
<h2 id="问题定义">问题定义</h2>
<p>考虑图（例如引文网络）中节点（例如文档）的分类问题，假设该图中只有一小部分节点标签（label）是已知的，我们的分类任务是想通过这部分已知标签的节点和图的结构来推断另一部分未知标签的节点的标签。这类问题可以划分到基于图结构数据的半监督学习问题中。半监督学习（semi-supervised learning）是有监督学习的一个分支，主要研究的是如何利用少量的有标签数据学习大量无标签数据的标签。</p>
<p>为了对节点进行分类，首先我们可以利用节点自身的特征信息，除此之外，我们还可以利用图结构信息，因此一个典型的图半监督学习问题可以对两个损失函数一起优化：L=Llabeled+λLreg(1)(1)L=Llabeled+λLreg</p>
<p>其中， LlabeledLlabeled 基于基于标签数据的损失函数（loss function），LregLreg 代表基于图结构信息的损失函数，λλ 是调节这两种损失函数相对重要性的超参（hyperparameter）。</p>
<p>一般来说，基于图结构信息的损失函数可以表示成：Lreg=∑i,jAij∥∥f(Xi)−f(Xj)∥∥2=f(X)⊤Δf(X)(2)(2)Lreg=∑i,jAij‖f(Xi)−f(Xj)‖2=f(X)⊤Δf(X)</p>
<p>其中，f(⋅)f(⋅) 是类似神经网络的可微分函数， 在无向图G=(V,E)G=(V,E) 中，假设有NN个节点vi∈Vvi∈V，XX为节点特征向量构成的矩阵，其中XiXi表示节点vivi的特征向量，边(vi,vj)∈E(vi,vj)∈E， 邻接矩阵表示为A∈RN×NA∈RN×N（可以是二值的(bianry)，也可以是加权的(weighted)），度矩阵Dii=∑jAijDii=∑jAij。Δ=D−AΔ=D−A表示无向图G=(V,E)G=(V,E)的未正则化图拉普拉斯算子。这样的损失函数希望对相邻节点的特征向量做限制，希望它们能尽量相近。</p>
<p>显然，这样的学习策略基于图中的相邻节点标签可能相同的假设（因为损失函数要求相邻节点的特征向量尽量相似，如何他们的标签不相似的话，那么不能学习到一个从特征向量到标签的有效映射）。然而，这个假设可能会限制模型的能力，因为图的边在语义上不一定代表所连接节点相似。还有一种可能是图中有大量的噪声边。</p>
<p>因此，在这个工作中，作者不再显示的定义图结构信息的损失函数 LregLreg , 而是使用神经网络模型f(X,A)f(X,A)直接对图结构进行编码，训练所有带标签的结点L0L0，来避免损失函数中的正则化项LregLreg。</p>
<p>这篇文章的主要贡献是为图半监督分类任务设计了一个简单并且效果好的神经网络模型，且这个模型由谱图卷积(spectral graph convolution)的一阶近似推导而来，具有理论基础。</p>
<h2 id="图上的快速卷积近似">图上的快速卷积近似</h2>
<blockquote>
<p>这一节介绍如何从谱图卷积推导出GCN的逐层更新模型，涉及到一些谱图理论的知识，可以安全的跳过这一节，后面我们会为谱图卷积专门出一个专栏的文章，将详细讨论它们</p>
</blockquote>
<p>这一节主要介绍图卷积网络GCN逐层更新(propagation)的理论推导。多层图卷积网络(Graph Convolutional Network, GCN)的逐层传播公式：</p>
<p>H(l+1)=σ(~D−12~A~D−12H(l)W(l))(1)(1)H(l+1)=σ(D~−12A~D~−12H(l)W(l))</p>
<p>其中，~A=A+INA~=A+IN表示无向图GG加上了自连接后的邻接矩阵，~Dii=∑j~AijD~ii=∑jA~ij是每个节点的度，W(l)W(l)是ll层可学习的参数。H(l)∈RN×DH(l)∈RN×D是第ll层激活后的节点Embedding值，输入层H(0)=XH(0)=X. 下面我们会重点介绍这个逐层的更新规则可以由<strong>图上的局部谱滤波(Localized spectral filters)的一阶近似</strong>推导而来。(另一种解释是基于Weisfeiler-Lehman算法的，见论文的附录A，之后也会有一篇文章详细讨论这个算法)</p>
<h3 id="谱图卷积spectral-graph-convolutions">谱图卷积(Spectral Graph Convolutions)</h3>
<p>谱图卷积是这样定义的：信号x∈RNx∈RN(对每个节点来说是一个标量，可以看成是节点的标签)通过一个傅里叶域的滤波器gθ=diag(θ),θ∈RNgθ=diag(θ),θ∈RN得到的结果，即：</p>
<p>gθ⋆x=UgθU⊤x(2)(2)gθ⋆x=UgθU⊤x</p>
<p>左式表示信号xx经傅里叶域滤波器gθgθ变换，右边是将这个变换过程用矩阵的乘法表示。这里要引入一个叫归一化的图拉普拉斯矩阵(normalized graph Laplacian)的概念，它可以表示为 L=IN−D−12AD−12=UΛU⊤L=IN−D−12AD−12=UΛU⊤，其中UU是LL的特征向量矩阵，而ΛΛ是对应的特征值矩阵(特征值在对角线上)，U⊤xU⊤x是xx在图上的傅里叶变换。我们能将gθgθ理解为LL特征值的一个函数，即gθ(Λ)gθ(Λ)。上面的等式算起来非常耗时，因为特征向量矩阵的乘法是O(N2)O(N2)阶的。再者，LL的特征分解在大图上也很低效。为了克服这样的问题 <a href="https://hal.inria.fr/inria-00541855/document">Hammond et al. (2011)</a>指出gθ(Λ)gθ(Λ)能够被切比雪夫多项式(Chebyshev polynomials) Tk(x)Tk(x)所近似，KK阶多项式就能达到很好的近似效果：</p>
<p>gθ′(Λ)≈K∑k=0θ′kTk(~Λ)(3)(3)gθ′(Λ)≈∑k=0Kθk′Tk(Λ~)</p>
<p>其中~Λ=2λmaxΛ−INΛ~=2λmaxΛ−IN. λmaxλmax表示LL的最大特征值。θ′∈Rkθ′∈Rk是切比雪夫因子，切比雪夫多项式是由Tk(x)=2xTk−1(x)−Tk−2(x)Tk(x)=2xTk−1(x)−Tk−2(x)递归定义的，其中T0(x)=1,T1(x)=xT0(x)=1,T1(x)=x。</p>
<p>将切比雪夫近似带入到谱图卷积的公式里：</p>
<p>gθ′⋆x≈K∑k=0θ′kTk(~L)x(4)(4)gθ′⋆x≈∑k=0Kθk′Tk(L~)x</p>
<p>其中~L=2λmaxL−INL~=2λmaxL−IN, 这个表达式现在是K-localized的，因为我们仅用KK阶切比雪夫多项式近似gθ(Λ)gθ(Λ)。即每个节点只受KK步以内的其他节点影响，评估上式的时间复杂度是 O(|E|)O(|E|) , 即线性于边的数量。<a href="https://arxiv.org/abs/1606.09375">Defferrard et al.2016</a> 使用 K-localized 卷积定义图上的卷积操作。</p>
<h3 id="逐层线性模型">逐层线性模型</h3>
<p>现在假设我们限制K=1K=1，即谱图卷积近似为一个关于LL的线性函数。这种情况下，我们仍能通过堆叠多层来得到卷积的能力，但是这时候，我们不再受限于切比雪夫多项式参数的限制。我们期望这样的模型能够缓解当图中节点度分布差异较大时对局部结构过拟合问题，比如社交网络，引文网络，知识图谱等。另一方面，从计算的角度考虑，逐层线性模型使我们可以构建更深的模型。</p>
<p>在GCN模型中， 我们做了另一个近似λmax≈2λmax≈2, 我们期望神经网络的参数能够在训练过程中适应这种人为设定的近似。在这些近似的条件下，我们得到：</p>
<p>gθ′⋆x≈θ′0x+θ′1(L−IN)x=θ′0x−θ′1D−12AD−12x(5)(5)gθ′⋆x≈θ0′x+θ1′(L−IN)x=θ0′x−θ1′D−12AD−12x</p>
<p>上式有两个参数θ′0θ0′和θ′1θ1′。这两个参数可以在整个图所有节点的计算中共享。在实践中，进一步限制参数的个数能够一定程度上避免过拟合的问题，并且减少计算量。因此我们引入θ=θ′0=−θ′1θ=θ0′=−θ1′做进一步的近似：</p>
<p>gθ⋆x≈θ(IN+D−12AD−12)x(6)(6)gθ⋆x≈θ(IN+D−12AD−12)x</p>
<p>注意IN+D−12AD−12IN+D−12AD−12的特征值现在被限制在了[0,2][0,2]中。重复这样的操作将会导致数值不稳定、梯度弥散/爆炸等问题。为了缓解这样的问题，我们引入了这样的再正则化(renormalization)技巧：IN+D−12AD−12→~D−12~A~D−12IN+D−12AD−12→D~−12A~D~−12，其中~A=A+IN,~Dii=∑j~AijA~=A+IN,D~ii=∑jA~ij</p>
<p>我们把上述定义进一步泛化：输入信号X∈RN×CX∈RN×C, 每个输入信号有CC个通道(channels), 即每个图有NN个节点，每个节点有CC维特征，图上的卷积操作包含FF个滤波器(filters)或特征映射(feature maps)，如下：</p>
<p>Z=~D−12~A~D−12XΘ(7)(7)Z=D~−12A~D~−12XΘ</p>
<p>其中Θ∈RC×FΘ∈RC×F 是filters的参数矩阵, Z∈RN×FZ∈RN×F 是卷积之后的信号矩阵。这样的转化操作的时间复杂度为O(|E|FC)O(|E|FC), 因为~AXA~X能够被高效的以稀疏矩阵和稠密矩阵相乘的形式实现。</p>
<p>XX是输入向量，神经网络通常有多层，我们习惯于把XX变换后的向量记做HH，表示节点在隐藏层的embedding, 其次，我们习惯将神经网络的参数表示为WW而非ΘΘ，输出ZZ会变成下一层的输入，做完这些符号替换后，多层图神经网络的Embedding更新公式为 H(l+1)=σ(~D−12~A~D−12H(l)W(l))H(l+1)=σ(D~−12A~D~−12H(l)W(l))</p>
<p>再次说明一下，上面的推导看起来复杂而不详细，这是故意而为之的，GCN的理论推导理论性很强且复杂，如果再次长篇大论很容易就打消了大家学习GNN的兴趣，因此我们特意把它安排到了几个专栏文章的后面，待大家熟悉了GNN的常见模型之后再介绍详细的推导更容易接受和理解，所以如果对上面的推导看得一知半解也没有关系，主要记住H(l+1)=σ(~D−12~A~D−12H(l)W(l))H(l+1)=σ(D~−12A~D~−12H(l)W(l))这个逐层传播公式就可以了。</p>
<h2 id="半监督学习节点分类">半监督学习节点分类</h2>
<h2 id="传播公式解释">传播公式解释</h2>
<p>上一节中，我们从谱图卷积理论中推导得到了GCN是如何逐层更新节点embedding的</p>
<p>H(l+1)=σ(~D−12~A~D−12H(l)W(l))(8)(8)H(l+1)=σ(D~−12A~D~−12H(l)W(l))</p>
<p>首先我对这个公式做一下形象的解释：<strong>每个节点拿到邻居节点信息然后聚合到自身embedding上</strong>。在上面的公式中D−12~AD−12D−12A~D−12可以看成是归一化后的邻接矩阵，H(l)W(l)H(l)W(l)相当于给ll层所有节点的embedding做了一次线性变换，左乘邻接矩阵表示对于每个节点来说，该节点的特征变为邻居节点特征相加后的结果。</p>
<p>这个形象的解释对理解GNN非常重要，希望大家能仔细想一下是不是懂了。</p>
<h3 id="例子">例子</h3>
<p>下面，让我们用一个两层GCN的例子阐述GCN是如何对节点进行分类的，令^A=~D−12~A~D−12A^=D~−12A~D~−12。根据逐层传播公式，这个两层的GCN输出embedding计算为：</p>
<p>Z=f(X,A)=softmax(^AReLU(^AXW(0))W(1))(9)(9)Z=f(X,A)=softmax⁡(A^ReLU⁡(A^XW(0))W(1))</p>
<p>这里，W(0)∈RH×CW(0)∈RH×C是权重矩阵, 目的是对节点的输入embeddingXX做线性变换(从输入层到隐藏层的变换)。W(1)∈RH×FW(1)∈RH×F是另一个权重矩阵，目的是对第一层变化后的节点embeding再做一次变换(从隐藏层到输出层)。变换结果经过softmax 激活函数后输出作为节点的分类结果。对于半监督多分类问题，我们在所有带标签的样本上评估交叉熵：</p>
<p>L=−∑l∈YLF∑f=1YlflnZlf(10)(10)L=−∑l∈YL∑f=1FYlfln⁡Zlf</p>
<p>这里YLYL是所有带标签节点的集合。最后通过利用梯度下降法训练神经网络权重W(0)W(0)和W(1)W(1) 就可以了。</p>
<h2 id="后话">后话</h2>
<p>实现时，由于GCN需要输入整个邻接矩阵AA和特征矩阵XX, 因此它是非常耗内存的，论文中作者做了优化，他们将AA作为稀疏矩阵输入，然后通过实现稀疏矩阵和稠密矩阵相乘的GPU算子来加速计算，然而，即使这样，整个矩阵仍然存在这要被塞进内存和显存中的问题，当图规模变大的时候，这种方法是不可取的，在下一篇GraphSAGE的博文中，我们将会介绍如何巧妙的克服这样的问题。</p>
<h2 id="reference">Reference</h2>
<p>[Semi-Supervised Classification with Graph Convolutional Networks](</p>

        </div>

        


        

<div class="post-archive">
    <h2>See Also</h2>
    <ul class="listing">
        
        <li><a href="/about/">About</a></li>
        
        <li><a href="/archives/">Archives</a></li>
        
        <li><a href="/post/math-typesetting/">Math Typesetting</a></li>
        
        <li><a href="/post/kenttest/">Career Roadmap</a></li>
        
        <li><a href="/post/workflow-of-hugoblog/">WorkFlow of HUGO Blog</a></li>
        
    </ul>
</div>


        <div class="post-meta meta-tags">
            
            No Tags
            
        </div>
    </article>
    
    <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "kentchun" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

    
    
</div>

                    <footer id="footer">
    <div>
        &copy; 2021 <a href="https://kentchun33333.github.io">Autumn Memo By Kent Chiu</a>
        
    </div>
    <br />
 
</footer>


    
    <script type="text/javascript">
        window.MathJax = {
            tex2jax: {
                inlineMath: [['$', '$']],
                processEscapes: true
                }
            };
    </script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>

<a id="rocket" href="#top"></a>
<script type="text/javascript" src='/js/totop.js?v=0.0.0' async=""></script>



    <script type="text/javascript" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script>





                </div>

                <div id="secondary">
    <section class="widget">
        <form id="search" action='https://kentchun33333.github.io/search/' method="get" accept-charset="utf-8" target="_blank" _lpchecked="1">
      
      <input type="text" name="q" maxlength="20" placeholder="Search">
      <input type="hidden" name="sitesearch" value="https://kentchun33333.github.io">
      <button type="submit" class="submit icon-search"></button>
</form>
    </section>
    
    <section class="widget">
        <h3 class="widget-title">Recent Post</h3>
<ul class="widget-list">
    
    <li>
        <a href="https://kentchun33333.github.io/post/math-typesetting/" title="Math Typesetting">Math Typesetting</a>
    </li>
    
    <li>
        <a href="https://kentchun33333.github.io/post/kenttest/" title="Career Roadmap">Career Roadmap</a>
    </li>
    
    <li>
        <a href="https://kentchun33333.github.io/post/workflow-of-hugoblog/" title="WorkFlow of HUGO Blog">WorkFlow of HUGO Blog</a>
    </li>
    
</ul>
    </section>

    

    <section class="widget">
        <h3 class="widget-title"><a href='/categories/'>Category</a></h3>
<ul class="widget-list">
    
</ul>
    </section>

    <section class="widget">
        <h3 class="widget-title"><a href='/tags/'>Tags</a></h3>
<div class="tagcloud">
    
    <a href="https://kentchun33333.github.io/tags/about-us/">about-us</a>
    
    <a href="https://kentchun33333.github.io/tags/contact/">contact</a>
    
    <a href="https://kentchun33333.github.io/tags/index/">index</a>
    
    <a href="https://kentchun33333.github.io/tags/roadmap/">Roadmap</a>
    
    <a href="https://kentchun33333.github.io/tags/workflow/">Workflow</a>
    
</div>
    </section>

    

    <section class="widget">
        <h3 class="widget-title">Others</h3>
        <ul class="widget-list">
            <li><a href="https://kentchun33333.github.io/index.xml">RSS</a></li>
        </ul>
    </section>
</div>
            </div>
        </div>
    </div>
</body>

</html>